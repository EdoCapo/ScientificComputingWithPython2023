{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Vodafone users' fluxes\n",
    "\n",
    "The study of the flux of people inside urban areas is of paramount importance to achieve an optimal understanding of emerging critical issues in the local mobility, and to explore areas of potential improvements in the infrastructures and local transports.\n",
    "\n",
    "The mobility of users within and toward Padova has been monitored using the data provided by the Vodafone mobile carrier, which provides the information based on the users' connections to the network cells.\n",
    "The data provided by the carrier encompasses the monitoring of the users connected to the Vodafone network in Padova in a four-month period from February to May of 2018.\n",
    "\n",
    "To provide statistical insights on the number and the flow of users, the data is aggregated based on the origin and movements of the users by averaging the number of connections during the time of the monitoring.\n",
    "\n",
    "To further avoid privacy violation issues, all observations with less than 30 units (e.g. day-areas for which $<$ 30 users have contributed) have been discarded and/or merged into dedicated categories (indicated with \"altro\", or \"other\").\n",
    "\n",
    "\n",
    "## Datasets \n",
    "\n",
    "The data is provided in `.csv` files.\n",
    "\n",
    "* __day_od.csv__: table of the origins and destinations of the users averaged by the day of the week. The data is provided with details of the month, type of user (resident in Padova/Italian visitor/foreign visitor), country of provenance, together with the province and comune of the user (if available).\n",
    "* __distinct_users_day.csv__: table of the number of distinct users by origin. The data is provided with details of the month, type of user (resident in Padova/Italian visitor/foreign visitor), country of provenance, together with the province and comune of the user (if available).\n",
    "\n",
    "The information is stored in the fields according to the following scheme: \n",
    "\n",
    "- __MONTH__: month analyzed\n",
    "- __DOW__: day analyzed\n",
    "- __ORIGIN__: users' origin area (do not consider this field)\n",
    "- __DESTINATION__: users' destination area (do not consider this field)\n",
    "- __CUST_CLASS__: user type (resident / Italian visitor / foreigner visitor)\n",
    "- __COD_COUNTRY__: users' country code (e.g. 222=Italy)\n",
    "- __COD_PRO__: users' province code (e.g. 12=Varese) \n",
    "- __PRO_COM__: users' comune code (e.g. 12026=Busto Arsizio)\n",
    "- __FLOW__: number of movements for given date-time (with a minimum of 30 users)\n",
    "- __VISITORS__: overall number of users \n",
    "\n",
    "Together with the data files, three lookup-tables are provided to allow matching the Italian institute of STATistics (ISTAT) country, province and comune codes to the actual names.\n",
    "\n",
    "* __codici_istat_comune.csv__: lookup file containing the mapping between _comune_ ISTAT code-names\n",
    "* __codici_istat_provincia.csv__: lookup file containing the mapping between _province_ ISTAT code-names\n",
    "* __codici_nazioni.csv__: lookup file containing mapping the _country_ code to its name\n",
    "\n",
    "Additional information, useful for the study of the flow of users, as the number of inhabitants of each province and the distance between Padova and all other Italian provinces can be extracted based on the data collected by the ISTAT:\n",
    "\n",
    "   - English: https://www.istat.it/en/analysis-and-products/databases, Italian: https://www.istat.it/it/dati-analisi-e-prodotti/banche-dati\n",
    "   \n",
    "   - English/Italian: https://www.istat.it/en/archive/157423, Italian: https://www.istat.it/it/archivio/157423\n",
    "   \n",
    "   - `.zip` package containing the distances between comuni in Veneto region: http://www.istat.it/storage/cartografia/matrici_distanze/Veneto.zip\n",
    "\n",
    "If deemed useful, the open repository [https://github.com/openpolis/geojson-italy](https://github.com/openpolis/geojson-italy) contains a `.json` file with the geographical coordinates of the provences and comuni of Italy.\n",
    "\n",
    "\n",
    "## Assignments\n",
    "\n",
    "1. Data preparation: the csv files are originated from different sources, hence resulting in differences in the encoding and end-of-lines that have to be taken into account in the data preparation phase. Make sure each .csv file is properly interpreted.\n",
    "\n",
    "   1.1 Ranking of visitors from foreign countries: based on the number of total visitors per each country, create a ranked plot of the first 20 countries with the most visitors\n",
    "   \n",
    "   1.2 Ranking of Italian visitors by province, weighted by the number of inhabitants: based on the number of total visitors per Italian province, create a ranked plot of the first 20 provinces with the most visitors taking into account the number of inhabitants.\n",
    "\n",
    "\n",
    "2. Study of the visitors' fluxes: you are asked to provide indications on how to invest resources to improve the mobility towards Padova. Consider the three main directions of visitors and commuters getting to Padova through the main highways (from south, A13 towards Bologna-Roma; from west, A4 towards Milano-Torino; from north-east, A4 towards Venice-Trieste). Evaluate which of the three directions has to be prioritized.\n",
    "\n",
    "   2.1 Consider a simplified case involving only the mid-range mobility, based on the number of visitors/commuters from the nearby regions only\n",
    "   \n",
    "   2.2 Consider the provinces located on the three directions that are mostly contributing to the flow of weekend visitors and working daily commuters by performing a more detailed study of the fluxes based on the day of the week. Use the data available to provide what you believe is the best possible answer.\n",
    "\n",
    "\n",
    "3. Plot the distribution of the number of visitors by the distance of the province of origin. Determine which kind of function should be used to describe the distribution.\n",
    "\n",
    "   3.1 Assuming an analytic form can be used to describe the trend, create a regression or a fit to estimate the expected number of visitors by the distance of the province of origin and the corresponding uncertainties. Illustrate the difference between the resulting regression with respect to the numbers provided by the Vodafone monitoring, and highlight the five most striking discrepancies from the expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet #Used to detect the encoding of the CSV files\n",
    "import codecs  #Used to read the CSV UTF-16\n",
    "import io      #Used to write the CSV ISO-8859-1\n",
    "import pandas as pd #Used to store data into dataframes\n",
    "import matplotlib.pyplot as plt #Used to represent data\n",
    "import numpy as np #Used to rename the column of dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of csv files\n",
    "filename_codici_istat_comuni = \"data\\codici_istat_comune.csv\"\n",
    "filename_codici_istat_provincia = \"data\\codici_istat_provincia.csv\"\n",
    "filename_codici_nazioni = \"data\\codici_nazioni.csv\"\n",
    "filename_day_od = \"data\\day_od.csv\"\n",
    "filename_distinct_user_day = \"data\\distinct_users_day.csv\"\n",
    "filename_distance_to_pd = \"data\\R05_PD.csv\"\n",
    "filename_distance_to_pd_txt = \"data\\R05_PD.txt\"\n",
    "\n",
    "#Creating a list to boost performances of the loops\n",
    "filenames = [filename_codici_istat_comuni, filename_codici_istat_provincia, filename_codici_nazioni,\n",
    "             filename_day_od, filename_distinct_user_day, filename_distance_to_pd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Function which returns the encoding of each csv file\n",
    "def check_encoding(file):\n",
    "    #Read the file\n",
    "    with open(file, 'rb') as f:\n",
    "        #Detect the encoding\n",
    "        result = chardet.detect(f.read())\n",
    "\n",
    "    #Return a list of the encodings\n",
    "    return result['encoding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Function wich converts the UTF-16 encoded files into ISO-8859-1 encoded files\n",
    "def encoding_converter(files):\n",
    "    for file in files:\n",
    "        #Saving the encoding of each file\n",
    "        encodings = check_encoding(file)\n",
    "\n",
    "        # If the encoding is different to ISO-8859-1 it has to be converted\n",
    "        if encodings == 'ascii' :\n",
    "            # Open the file and saving the content\n",
    "            with codecs.open(file, 'r', 'ascii') as f:\n",
    "                data = f.read()\n",
    "\n",
    "            # Overwrite the file with a new encoding\n",
    "            with io.open(file, 'w', encoding='utf-8') as f:\n",
    "                f.write(data)\n",
    "\n",
    "        if encodings == 'utf-16':\n",
    "            # Open the file and saving the content\n",
    "            with codecs.open(file, 'r', 'utf-16') as f:\n",
    "                data = f.read()\n",
    "\n",
    "            # Overwrite the file with a new encoding\n",
    "            with io.open(file, 'w', encoding='latin1') as f:\n",
    "                f.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Ranking of visitors from foreign countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Ranking of visitors from Italy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Study of the visitors' fluxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Praticamente bisogna controllare tutti i caselli autostradali delle 3 autostrade\n",
    "Sommare tutti i flows attraverso i comuni delle 3 autostrade\n",
    "Potrei plottare in base al casello, per ogni autostrada\n",
    "I codici delle persone intendono l'origine delle persone\n",
    "In realtà è una semplificazione perchè immagino che tutti quelli del nordest abbiano preso l'autostrada A4. Farò così:\n",
    "Suddivido in 4 zone: \n",
    "1. NordEst (A4 To-Mi) ad Est di Padova\n",
    "2. NordOvest (A4 Ts-Ve)\n",
    "3. La parte sinistra dell'Italia prende l'A1 che poi diventa A13\n",
    "4. La parte destra dell'Italia prende direttamente l'A13 (Emilia Marche Abruzzo Molise Puglia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toll_booths(highway = \"\"):   \n",
    "    if highway == \"A4To-Mi\" :\n",
    "        data_tolls = pd.DataFrame({'COMUNE':[\"Torino\", \"Borgo d'Ale\", \"Santhià\", \"Carisio\", \"Balocco\", \"Greggio\", \"Biandrate\", \"Novara\",\n",
    "            \"Mesero\", \"Arluno\", \"Rho\", \"Milano\", \"Monza\", \"Agrate\", \"Cavenago\", \"Trezzo\", \"Capriate\", \"Dalmine\",\n",
    "            \"Bergamo\", \"Seriate\", \"Grumello\", \"Ponte Oglio\", \"Palazzolo\", \"Rovato\", \"Ospitaletto\", \"Castegnato\",\n",
    "            \"Brescia\", \"Desenzano\", \"Sirmione\", \"Peschiera\", \"Sommacampagna\", \"Verona\", \"Soave\", \"Montebello\",\n",
    "            \"Montecchio\", \"Vicenza\", \"Grisignano\"]})\n",
    "        return data_tolls\n",
    "        \n",
    "    elif highway == \"A4Ts-Ve\":\n",
    "        data_tolls = pd.DataFrame({'COMUNE':[\"Spinea\", \"Martellago\", \"Preganziol\", \"Meolo\", \"San Donà di Piave\", \"Cessalto\",\n",
    "            \"San Stino di Livenza\", \"Latisana\", \"San Giorgio di Nogaro\", \"Palmanova\", \"Redipuglia\", \"Trieste\"]})\n",
    "        return data_tolls\n",
    "\n",
    "    elif highway == \"A1Ro-Bo\":\n",
    "        data_tolls = pd.DataFrame({'COMUNE':[\"Sasso Marconi\", \"Rioveggio\", \"Pian del Voglio\", \"Roncobilaccio\", \"Barberino di Mugello\",\n",
    "            \"Calenzano\", \"Firenze\", \"Incisa\", \"Valdarno\", \"Arezzo\", \"Monte San Savino\", \"Valdichiana\", \"Chiusi\",\n",
    "            \"Fabro\", \"Orvieto\", \"Attigliano\", \"Orte\", \"Magliano Sabina\", \"Ponzano Romano\", \"Guidonia Montecelio\",\n",
    "            \"Valmontone\", \"Colleferro\", \"Anagni\"]})\n",
    "        return data_tolls\n",
    "\n",
    "    elif highway == \"A13Bo-Pd\":\n",
    "        data_tolls = pd.DataFrame({'COMUNE':[\"Bologna\", \"Altedo\", \"Ferrara\", \"Occhiobello\", \"Rovigo\", \"Boara\", \"Monselice\", \"Terme Euganee\"]})\n",
    "        return data_tolls\n",
    "    \n",
    "    elif highway == \"Vi-Ve\": #Da fare\n",
    "        data_tolls = pd.DataFrame({'COMUNE':[\"Bologna\", \"Altedo\", \"Ferrara\", \"Occhiobello\", \"Rovigo\", \"Boara\", \"Monselice\", \"Terme Euganee\"]})\n",
    "        return data_tolls\n",
    "    \n",
    "    elif highway == \"Bl-Tv-Ve\": #Da fare\n",
    "        data_tolls = pd.DataFrame({'COMUNE':[\"Bologna\", \"Altedo\", \"Ferrara\", \"Occhiobello\", \"Rovigo\", \"Boara\", \"Monselice\", \"Terme Euganee\"]})\n",
    "        return data_tolls\n",
    "        \n",
    "    #Da fare\n",
    "    data_tolls = pd.DataFrame({'COMUNE':[\"Bologna\", \"Altedo\", \"Ferrara\", \"Occhiobello\", \"Rovigo\", \"Boara\", \"Monselice\", \"Terme Euganee\"]})\n",
    "    return data_tolls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def possible_highway(data):\n",
    "\n",
    "    #Ignoro Padova perchè non prendono l'autostrada\n",
    "    data_no = data.loc[(data['REGION'] == \"Lombardia\") | (data['REGION'] == \"Trentino-Alta Adige\") | (data['REGION'] == \"Valle d'Aosta\")\n",
    "                       | (data['REGION'] == \"Piemonte\") | (data['REGION'] == \"Liguria\")]\n",
    "    data_ne = data.loc[(data['REGION'] == \"Friuli-Venezia Giulia\")]\n",
    "    data_so = data.loc[(data['REGION'] == \"Emilia Romagna\") | (data['REGION'] == \"Marche\") | (data['REGION'] == \"Abruzzo\")\n",
    "                       | (data['REGION'] == \"Molise\") | (data['REGION'] == \"Puglia\")]\n",
    "    data_se = data.loc[(data['REGION'] == \"Toscana\") | (data['REGION'] == \"Umbria\") | (data['REGION'] == \"Lazio\")\n",
    "                       | (data['REGION'] == \"Campania\") | (data['REGION'] == \"Basilicata\") | (data['REGION'] == \"Calabria\")]\n",
    "    data_ve = data.loc[(data['REGION'] == \"VENETO\") & ((data['COD_PRO'] == 24) | (data['COD_PRO'] == 23))] #Vicenza e Verona\n",
    "    data_vo = data.loc[(data['REGION'] == \"VENETO\") & ((data['COD_PRO'] == 25)| (data['COD_PRO'] == 26) | (data['COD_PRO'] == 27))] #Belluno, Treviso e Venezia\n",
    "    data_ro = data.loc[(data['REGION'] == \"VENETO\") & (data['COD_PRO'] == 29)] #Rovigo\n",
    "                       \n",
    "    data_groups  = [data_no, data_ne, data_so, data_se, data_ve, data_vo, data_ro]\n",
    "    list_highways = [\"A4To-Mi\", \"A4Ts-Ve\", \"A13Bo-Pd\", \"A1Ro-Bo\", \"Vi-Ve\", \"Bl-Ve-Tv\", \"Ro\"] \n",
    "\n",
    "    for x in range(0,len(data_groups)):\n",
    "        # CALCOLO DEI FLUSSI DEI COMUNI CON CASELLO\n",
    "        comuni_toll = toll_booths(list_highways[x])\n",
    "        #Controllo corrispondenze, se trovo corrispondenze calcolo la somma\n",
    "        data_comuni_toll = pd.merge(data_groups[x], comuni_toll, on=\"COMUNE\")\n",
    "\n",
    "        # CALCOLO DEI FLUSSI DELLE PROVINCE CON CASELLO\n",
    "        #Altrimenti controllo la provincia\n",
    "        comuni_codes = pd.read_csv(filename_codici_istat_comuni, encoding='latin1', usecols=['COMUNE', 'PRO_COM', 'COD_PRO', 'COMUNE_CAPOLUOGO'])\n",
    "        comuni_toll = pd.merge(comuni_toll, comuni_codes, on=\"COMUNE\")\n",
    "        # Subtraction between two dataframe\n",
    "        data_without_comuni_toll = data_groups[x].merge(data_comuni_toll, how='outer', indicator=True).query('_merge == \"left_only\"').drop('_merge', axis=1)\n",
    "        # Prendo le province dei comuni con i caselli\n",
    "        data_province_toll = data_without_comuni_toll.drop(['COMUNE','PRO_COM'], axis= 1)\n",
    "        data_province_toll = pd.merge(data_province_toll, comuni_toll, on='COD_PRO')\n",
    "        data_province_toll = data_province_toll.groupby(['COD_PRO'])[['VISITORS']].sum()\n",
    "\n",
    "        # Unire i due DataFrame in base alla colonna 'COD_PRO'\n",
    "        data_com_pro_toll = pd.merge(data_province_toll, data_comuni_toll, suffixes=('_df1', '_df2'), on='COD_PRO', how='outer')\n",
    "\n",
    "        # Sommare i valori della colonna 'visitors' dei due DataFrame\n",
    "        data_com_pro_toll['VISITORS'] = data_com_pro_toll['VISITORS_df1'].fillna(0) + data_com_pro_toll['VISITORS_df2'].fillna(0)\n",
    "        data_com_pro_toll = data_com_pro_toll.drop(['VISITORS_df1','VISITORS_df2'], axis= 1)\n",
    "\n",
    "        data_com_pro_toll = data_com_pro_toll.dropna()\n",
    "\n",
    "        # CALCOLO DEI FLUSSI PER USER CHE NON HANNO UN CASELLO IN PROVINCIA -> DISTRIBUISCO UNIFORMEMENTE PER I CASELLI\n",
    "        # Prendo tutte le province del data_groups[x] -> Ci tolgo quelle con il casello\n",
    "        province = set(data_groups[x]['COD_PRO'].unique())\n",
    "        province_with_toll = set(data_com_pro_toll['COD_PRO'].unique())\n",
    "        province_without_toll = pd.DataFrame({'COD_PRO':list(province - province_with_toll)})\n",
    "        \n",
    "        data_region_toll = pd.merge(data_groups[x], province_without_toll, on=\"COD_PRO\")\n",
    "        \n",
    "        # Sommo il flusso delle province\n",
    "        data_region_toll = data_region_toll.groupby('COD_PRO')[['VISITORS']].sum()\n",
    "        # Uniformo\n",
    "        flow = (data_region_toll['VISITORS'].sum() / len(data_com_pro_toll)).round()\n",
    "        # Sommo\n",
    "        data_com_pro_toll['VISITORS'] = data_com_pro_toll['VISITORS'] + flow\n",
    "        \n",
    "    return data_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ecapo\\AppData\\Local\\Temp\\ipykernel_55012\\272080303.py:55: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  flow = (data_region_toll['VISITORS'].sum() / len(data_com_pro_toll)).round()\n",
      "C:\\Users\\ecapo\\AppData\\Local\\Temp\\ipykernel_55012\\272080303.py:55: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  flow = (data_region_toll['VISITORS'].sum() / len(data_com_pro_toll)).round()\n",
      "C:\\Users\\ecapo\\AppData\\Local\\Temp\\ipykernel_55012\\272080303.py:55: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  flow = (data_region_toll['VISITORS'].sum() / len(data_com_pro_toll)).round()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[       PRO_COM  VISITORS  COD_PRO              COMUNE  COD_REG     REGION\n",
       " 0       1002.0        36        1             Airasca        1   Piemonte\n",
       " 1       1008.0       268        1           Alpignano        1   Piemonte\n",
       " 2       1013.0       220        1           Avigliana        1   Piemonte\n",
       " 3       1024.0        36        1            Beinasco        1   Piemonte\n",
       " 4       1028.0       344        1    Borgaro Torinese        1   Piemonte\n",
       " ...        ...       ...      ...                 ...      ...        ...\n",
       " 2260  108048.0       308      108      Verano Brianza        3  Lombardia\n",
       " 2261  108049.0       568      108          Villasanta        3  Lombardia\n",
       " 2262  108050.0       904      108           Vimercate        3  Lombardia\n",
       " 2263  108053.0       496      108      Cornate d'Adda        3  Lombardia\n",
       " 2264  108054.0      1116      108  Lentate sul Seveso        3  Lombardia\n",
       " \n",
       " [727 rows x 6 columns],\n",
       "       PRO_COM  VISITORS  COD_PRO                   COMUNE  COD_REG  \\\n",
       " 1243  30001.0       424       30        Aiello del Friuli        6   \n",
       " 1244  30004.0       256       30                 Aquileia        6   \n",
       " 1245  30008.0       188       30            Bagnaria Arsa        6   \n",
       " 1246  30009.0       372       30                Basiliano        6   \n",
       " 1247  30010.0        32       30                 Bertiolo        6   \n",
       " ...       ...       ...      ...                      ...      ...   \n",
       " 2150  93041.0      1996       93  San Vito al Tagliamento        6   \n",
       " 2151  93043.0       708       93         Sesto al Reghena        6   \n",
       " 2152  93044.0      1340       93              Spilimbergo        6   \n",
       " 2153  93051.0       952       93                  Zoppola        6   \n",
       " 2154  93053.0       616       93         Valvasone Arzene        6   \n",
       " \n",
       "                      REGION  \n",
       " 1243  Friuli-Venezia Giulia  \n",
       " 1244  Friuli-Venezia Giulia  \n",
       " 1245  Friuli-Venezia Giulia  \n",
       " 1246  Friuli-Venezia Giulia  \n",
       " 1247  Friuli-Venezia Giulia  \n",
       " ...                     ...  \n",
       " 2150  Friuli-Venezia Giulia  \n",
       " 2151  Friuli-Venezia Giulia  \n",
       " 2152  Friuli-Venezia Giulia  \n",
       " 2153  Friuli-Venezia Giulia  \n",
       " 2154  Friuli-Venezia Giulia  \n",
       " \n",
       " [114 rows x 6 columns],\n",
       "        PRO_COM  VISITORS  COD_PRO                    COMUNE  COD_REG  \\\n",
       " 1325   33007.0       128       33                     Cadeo        8   \n",
       " 1326   33012.0        32       33           Castell'Arquato        8   \n",
       " 1327   33013.0       212       33       Castel San Giovanni        8   \n",
       " 1328   33018.0       152       33             Cortemaggiore        8   \n",
       " 1329   33021.0       276       33        Fiorenzuola d'Arda        8   \n",
       " ...        ...       ...      ...                       ...      ...   \n",
       " 2274  110003.0       576      110                 Bisceglie       16   \n",
       " 2275  110004.0       316      110          Canosa di Puglia       16   \n",
       " 2276  110006.0        32      110           Minervino Murge       16   \n",
       " 2277  110007.0       356      110  San Ferdinando di Puglia       16   \n",
       " 2278  110009.0      1116      110                     Trani       16   \n",
       " \n",
       "               REGION  \n",
       " 1325  Emilia Romagna  \n",
       " 1326  Emilia Romagna  \n",
       " 1327  Emilia Romagna  \n",
       " 1328  Emilia Romagna  \n",
       " 1329  Emilia Romagna  \n",
       " ...              ...  \n",
       " 2274          Puglia  \n",
       " 2275          Puglia  \n",
       " 2276          Puglia  \n",
       " 2277          Puglia  \n",
       " 2278          Puglia  \n",
       " \n",
       " [403 rows x 6 columns],\n",
       "        PRO_COM  VISITORS  COD_PRO         COMUNE  COD_REG    REGION\n",
       " 1565   45003.0      1000       45        Carrara        9   Toscana\n",
       " 1566   45010.0      1376       45          Massa        9   Toscana\n",
       " 1567   45013.0        64       45      Podenzana        9   Toscana\n",
       " 1568   45014.0       104       45     Pontremoli        9   Toscana\n",
       " 1569   46001.0       132       46     Altopascio        9   Toscana\n",
       " ...        ...       ...      ...            ...      ...       ...\n",
       " 2208  100005.0      5112      100          Prato        9   Toscana\n",
       " 2209  100006.0       220      100         Vaiano        9   Toscana\n",
       " 2210  101010.0       368      101        Crotone       18  Calabria\n",
       " 2211  102003.0       108      102       Briatico       18  Calabria\n",
       " 2212  102047.0       320      102  Vibo Valentia       18  Calabria\n",
       " \n",
       " [350 rows x 6 columns],\n",
       " Empty DataFrame\n",
       " Columns: [PRO_COM, VISITORS, COD_PRO, COMUNE, COD_REG, REGION]\n",
       " Index: [],\n",
       " Empty DataFrame\n",
       " Columns: [PRO_COM, VISITORS, COD_PRO, COMUNE, COD_REG, REGION]\n",
       " Index: [],\n",
       " Empty DataFrame\n",
       " Columns: [PRO_COM, VISITORS, COD_PRO, COMUNE, COD_REG, REGION]\n",
       " Index: []]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visitors_fluxes(filename_distinct_user_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_region(data):\n",
    "    regioni = {\n",
    "        'Abruzzo': 13,\n",
    "        'Basilicata': 17,\n",
    "        'Calabria': 18,\n",
    "        'Campania': 15,\n",
    "        'Emilia Romagna': 8,\n",
    "        'Friuli-Venezia Giulia': 6,\n",
    "        'Lazio': 12,\n",
    "        'Liguria': 7,\n",
    "        'Lombardia': 3,\n",
    "        'Marche': 11,\n",
    "        'Molise': 14,\n",
    "        'Piemonte': 1,\n",
    "        'Puglia': 16,\n",
    "        'Sardegna': 20,\n",
    "        'Sicilia': 19,\n",
    "        'Toscana': 9,\n",
    "        'Trentino-Alto Adige': 4,\n",
    "        'Umbria': 10,\n",
    "        'Valle d\\'Aosta': 2,\n",
    "        'Veneto': 5}\n",
    "    \n",
    "    # Inizializza una lista per memorizzare i nomi delle regioni\n",
    "    region_names = []\n",
    "    \n",
    "    # Itera attraverso ogni riga del DataFrame\n",
    "    for index, row in data.iterrows():\n",
    "        # Ottieni il codice della regione dalla colonna 'COD_REG'\n",
    "        cod_reg = row['COD_REG']\n",
    "        \n",
    "        # Cerca il nome della regione corrispondente al codice nella colonna 'COD_REG'\n",
    "        for region_name, region_code in regioni.items():\n",
    "            if cod_reg == region_code:\n",
    "                # Aggiungi il nome della regione alla lista\n",
    "                region_names.append(region_name)\n",
    "                break\n",
    "        else:\n",
    "            # Se il codice della regione non è presente nel dizionario 'regioni', aggiungi None\n",
    "            region_names.append(None)\n",
    "    \n",
    "    # Aggiungi la lista dei nomi delle regioni come nuova colonna 'REGIONE' nel DataFrame\n",
    "    data['REGION'] = region_names\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visitors_fluxes(file_customers, dow_study=False):\n",
    "    '''\n",
    "    Consider the three main directions:\n",
    "    A13 Roma-Bologna\n",
    "    A4 Torino-Milano\n",
    "    A4 Trieste-Venezia\n",
    "    '''\n",
    "    #Consider the istat province codes\n",
    "    comuni_codes = pd.read_csv(filename_codici_istat_comuni, encoding='latin1', usecols=['COMUNE', 'PRO_COM', 'COD_PRO'])\n",
    "    provinces_codes = pd.read_csv(filename_codici_istat_provincia, encoding='latin1', usecols=['COD_PRO', 'COD_REG'])\n",
    "\n",
    "    codes = pd.merge(comuni_codes, provinces_codes, on='COD_PRO')\n",
    "\n",
    "    if not dow_study:\n",
    "        #Now take the data of the customers and see what's their origin and their destination\n",
    "        customers_data = pd.read_csv(file_customers, encoding='latin1', usecols = ['VISITORS', 'PRO_COM'])\n",
    "    else:\n",
    "        customers_data = pd.read_csv(file_customers, encoding='latin1', usecols = ['VISITORS', 'DOW', 'PRO_COM'])\n",
    "\n",
    "    customers_data.dropna(axis=0, inplace=True)\n",
    "\n",
    "    user_regions = add_region(codes)\n",
    "    \n",
    "    #Join between customers_data and province_codes in this way filters the province_codes in which we're not interested\n",
    "    customers_data = customers_data.groupby('PRO_COM').sum()\n",
    "    \n",
    "    data = pd.merge(customers_data, user_regions, on='PRO_COM')\n",
    "\n",
    "    highways = possible_highway(data)\n",
    "\n",
    "    return highways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_visitors(data, range_of_mobility=\"\"):\n",
    "    #Counting the flows ov visitors from every city we're interested (both ways)\n",
    "    data = data.groupby(['PROVINCIA'])[['FLOW']].sum()\n",
    "\n",
    "    far = True\n",
    "\n",
    "    if range_of_mobility == \"nearby\":\n",
    "        far = False\n",
    "\n",
    "    #Saving the informations per highway\n",
    "    A13_Roma_Bologna = far * data.FLOW.loc['Roma'] + data.FLOW.loc['Bologna']\n",
    "    A4_Milano_Torino = data.FLOW.loc['Milano'] + far * data.FLOW.loc['Torino']\n",
    "    A4_Venezia_Trieste = data.FLOW.loc['Venezia'] + data.FLOW.loc['Trieste']\n",
    "\n",
    "    highway = [A13_Roma_Bologna, A4_Milano_Torino, A4_Venezia_Trieste]\n",
    "\n",
    "    #Labels for the plot\n",
    "    highway_labels = [\"A13 Roma-Bologna\", \"A4 Milano-Torino\", \"A4 Venezia-Trieste\"]\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(highway_labels, highway, color='skyblue')\n",
    "    plt.xlabel('Highways')\n",
    "    plt.ylabel('Visitors'' flow')\n",
    "    plt.title('Visitors'' flow per highway')\n",
    "    # Set the scale to avoid the exponential notation \n",
    "    plt.ticklabel_format(style='plain', axis='y')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Usecols do not match columns, columns expected but not found: ['VISITORS']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plot_visitors(\u001b[43mvisitors_fluxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_day_od\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[18], line 16\u001b[0m, in \u001b[0;36mvisitors_fluxes\u001b[1;34m(file_customers, dow_study)\u001b[0m\n\u001b[0;32m     12\u001b[0m codes \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(comuni_codes, provinces_codes, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCOD_PRO\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dow_study:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m#Now take the data of the customers and see what's their origin and their destination\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m     customers_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_customers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlatin1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mVISITORS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPRO_COM\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     18\u001b[0m     customers_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_customers, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m, usecols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVISITORS\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDOW\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPRO_COM\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1723\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1720\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1722\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1724\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1725\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:140\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musecols_dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mset\u001b[39m(usecols)\u001b[38;5;241m.\u001b[39missubset(\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_names\n\u001b[0;32m    139\u001b[0m ):\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_usecols_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morig_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlen\u001b[39m(usecols):  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\base_parser.py:969\u001b[0m, in \u001b[0;36mParserBase._validate_usecols_names\u001b[1;34m(self, usecols, names)\u001b[0m\n\u001b[0;32m    967\u001b[0m missing \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m usecols \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m names]\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 969\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    970\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsecols do not match columns, columns expected but not found: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    971\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    972\u001b[0m     )\n\u001b[0;32m    974\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m usecols\n",
      "\u001b[1;31mValueError\u001b[0m: Usecols do not match columns, columns expected but not found: ['VISITORS']"
     ]
    }
   ],
   "source": [
    "plot_visitors(visitors_fluxes(filename_day_od))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.b Mid-range mobility\n",
    "Mobility to/from nearby regions, which are Lombardia, Trentino Alto Adige, Friuli Venezia Giulia, Emilia Romagna.\n",
    "QUESTA SOLUZIONE TIENE CONTO DELLE AUTOSTRADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_visitors(visitors_fluxes(filename_day_od), \"nearby\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Week flow\n",
    "\n",
    "Consider the provinces located on the three directions that are mostly contributing to the flow of weekend visitors and working daily commuters by performing a more detailed study of the fluxes based on the day of the week. Use the data available to provide what you believe is the best possible answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(data1, data2, label):\n",
    "    #Saving the informations per highway\n",
    "    A13_Roma_Bologna = data1.FLOW.loc['Roma'] + data1.FLOW.loc['Bologna']\n",
    "    A4_Milano_Torino = data1.FLOW.loc['Milano'] + data1.FLOW.loc['Torino']\n",
    "    A4_Venezia_Trieste = data1.FLOW.loc['Venezia'] + data1.FLOW.loc['Trieste']\n",
    "\n",
    "    highway1 = [A13_Roma_Bologna, A4_Milano_Torino, A4_Venezia_Trieste]\n",
    "\n",
    "    #Saving the informations per highway\n",
    "    A13_Roma_Bologna = data2.FLOW.loc['Roma'] + data2.FLOW.loc['Bologna']\n",
    "    A4_Milano_Torino = data2.FLOW.loc['Milano'] + data2.FLOW.loc['Torino']\n",
    "    A4_Venezia_Trieste = data2.FLOW.loc['Venezia'] + data2.FLOW.loc['Trieste']\n",
    "\n",
    "    highway2 = [A13_Roma_Bologna, A4_Milano_Torino, A4_Venezia_Trieste]\n",
    "\n",
    "    #Labels for the plot\n",
    "    highway_labels = [\"A13 Roma-Bologna\", \"A4 Milano-Torino\", \"A4 Venezia-Trieste\"]\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    '''\n",
    "    plt.bar(highway_labels, highway, color='skyblue')\n",
    "    plt.xlabel(label)\n",
    "    plt.ylabel('Visitors'' flow')\n",
    "    plt.title('Visitors'' flow in ' + label)\n",
    "    # Set the scale to avoid the exponential notation \n",
    "    plt.ticklabel_format(style='plain', axis='y')\n",
    "    plt.tight_layout()\n",
    "    '''\n",
    "    # Plot dei valori\n",
    "    plt.plot(highway_labels, highway1, marker='o')  # 'marker' specifica il tipo di marker per i punti\n",
    "    plt.plot(highway_labels, highway2, marker='x')  # 'marker' specifica il tipo di marker per i punti\n",
    "    plt.xlabel('X')  # Etichetta asse x\n",
    "    plt.ylabel('Y')  # Etichetta asse y\n",
    "    plt.title('Plot tipo funzione dei valori di un DataFrame')  # Titolo del grafico\n",
    "    plt.grid(False)  # Abilita la griglia\n",
    "\n",
    "    plt.show()\n",
    "    #Fare istogramma grouped bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Seleziono i turisti per le tre direzioni, poi li suddivido in weekend e working days\n",
    "'''\n",
    "def dow_visitors_fluxes(data):\n",
    "    data_weekend = data.loc[(data['DOW'] == \"Domenica\") | (data['DOW'] == \"Sabato\")]\n",
    "\n",
    "    data_working_day = data.loc[~data.index.isin(data_weekend.index)]\n",
    "\n",
    "    # #Join between customers_data and province_codes in this way filters the province_codes in which we're not interested\n",
    "    # data_weekend = pd.merge(weekend_flow, data, on='COD_PRO')\n",
    "    # data_working_day = pd.merge(working_day_flow, data, on='COD_PRO')\n",
    "\n",
    "    #Counting the flows ov visitors from every city we're interested (both ways)\n",
    "    data_weekend = data_weekend.groupby(['PROVINCIA'])[['FLOW']].sum()\n",
    "    data_working_day = data_working_day.groupby(['PROVINCIA'])[['FLOW']].sum()\n",
    "\n",
    "    plotter(data_weekend, data_working_day, \"Weekend\")\n",
    "    # plotter(data_working_day, \"Working \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dow_visitors_fluxes(visitors_fluxes(filename_day_od))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3\n",
    "Plot the distribution of the number of visitors by the distance of the province of origin. Determine which kind of function should be used to describe the distribution.\n",
    "\n",
    "è scritto visitor quindi tolgo i non visitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_visitors_distance(province_data):\n",
    "    distances_data = pd.read_csv(province_data, sep=\"\\t\", encoding=\"UTF-8\", usecols=['DEST_PROCOM', 'KM_TOT'])\n",
    "\n",
    "    distances_data.rename(columns={'DEST_PROCOM': 'PRO_COM'}, inplace=True)\n",
    "\n",
    "    distances_data['KM_TOT'] = distances_data['KM_TOT'].str.replace(',','.')\n",
    "    distances_data['KM_TOT'] = pd.to_numeric(distances_data['KM_TOT'])\n",
    "\n",
    "    distances_data = distances_data.groupby(['PRO_COM'])[['KM_TOT']].mean()\n",
    "\n",
    "    customers_data = pd.read_csv(filename_distinct_user_day, encoding=\"latin1\", usecols=['PRO_COM','VISITORS','CUST_CLASS'])\n",
    "\n",
    "    # Escludo Padova\n",
    "    customers_data['PRO_COM'] = np.where(customers_data['PRO_COM'] == 28060.0 , np.nan,\n",
    "                                np.where(customers_data['CUST_CLASS'] != 'visitor', np.nan, customers_data['PRO_COM']))\n",
    "    customers_data.dropna(axis=0, inplace=True)\n",
    "\n",
    "    # Elimina la colonna \"CUST_CLASS\"\n",
    "    customers_data = customers_data.drop('CUST_CLASS', axis=1)\n",
    "\n",
    "    customers_data = customers_data.groupby(['PRO_COM'])[['VISITORS']].sum()\n",
    "\n",
    "    data = pd.merge(distances_data, customers_data, on='PRO_COM')\n",
    "\n",
    "    data = data.sort_values(['VISITORS'], ascending=False)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_visitors_distance(filename_distance_to_pd_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_visitors_distance(data, zoom=1):\n",
    "    # zoom serve a raggruppare le distanze (50 --> raggruppo le distanze ogni 50 km)\n",
    "    # calcolo il range di raggruppamento\n",
    "    period = int(data['KM_TOT'].max() / zoom)\n",
    "    somma_visitatori = []\n",
    "    for n_group in range(1,period+1):\n",
    "        data_grouped = data[data['KM_TOT'].between((n_group-1)*zoom +1 , n_group * zoom)]\n",
    "        somma_visitatori.append(data_grouped['VISITORS'].sum())\n",
    "    \n",
    "    data_to_plot = pd.DataFrame({'DISTANCE_TO_PD': [km * zoom for km in range(0, period)], 'VISITORS': somma_visitatori})\n",
    "    # Impostare l'indice dopo la creazione del DataFrame\n",
    "    data_to_plot.set_index('DISTANCE_TO_PD', inplace=True)\n",
    "\n",
    "    # data_to_plot['KM_TOT'] = np.where(data_to_plot['KM_TOT'] == 0 , np.nan, data_to_plot['KM_TOT'])\n",
    "    # data_to_plot.dropna(axis=0, inplace=True)\n",
    "\n",
    "    # data_nearby = data.loc[(data['KM_TOT'] < 50)]\n",
    "\n",
    "    # data_far = data.loc[~data.index.isin(data_nearby.index)]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(data_to_plot['VISITORS'], color='blue', alpha=0.7, bins=20)\n",
    "    plt.xlabel('Distance from Padova (KM)')\n",
    "    plt.ylabel('Number of Visitors')\n",
    "    plt.title('Histogram of Distance from Padova')\n",
    "    plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "    plt.show()\n",
    "    #Da riguardare il grafico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_visitors_distance(data_visitors_distance(filename_distance_to_pd_txt), 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 \n",
    "Assuming an analytic form can be used to describe the trend, create a regression or a fit to estimate the expected number of visitors by the distance of the province of origin and the corresponding uncertainties. Illustrate the difference between the resulting regression with respect to the numbers provided by the Vodafone monitoring, and highlight the five most striking discrepancies from the expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Build linear regression model using TV and Radio as predictors\n",
    "# Split data into predictors X and output Y\n",
    "X = np.array(data['KM_TOT']).reshape(-1,1)\n",
    "y = data['VISITORS']\n",
    "\n",
    "# Initialise and fit model\n",
    "lm = LinearRegression()\n",
    "model = lm.fit(X, y)\n",
    "# plot for residual error\n",
    " \n",
    "   \n",
    "plt.scatter(X,y,color='red')\n",
    "plt.plot(X,model.predict(X),color='green')\n",
    "plt.title('Simple Linear Regression')\n",
    "plt.xlabel('Position Level')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
