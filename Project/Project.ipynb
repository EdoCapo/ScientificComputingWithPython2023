{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Vodafone users' fluxes\n",
    "\n",
    "The study of the flux of people inside urban areas is of paramount importance to achieve an optimal understanding of emerging critical issues in the local mobility, and to explore areas of potential improvements in the infrastructures and local transports.\n",
    "\n",
    "The mobility of users within and toward Padova has been monitored using the data provided by the Vodafone mobile carrier, which provides the information based on the users' connections to the network cells.\n",
    "The data provided by the carrier encompasses the monitoring of the users connected to the Vodafone network in Padova in a four-month period from February to May of 2018.\n",
    "\n",
    "To provide statistical insights on the number and the flow of users, the data is aggregated based on the origin and movements of the users by averaging the number of connections during the time of the monitoring.\n",
    "\n",
    "To further avoid privacy violation issues, all observations with less than 30 units (e.g. day-areas for which $<$ 30 users have contributed) have been discarded and/or merged into dedicated categories (indicated with \"altro\", or \"other\").\n",
    "\n",
    "\n",
    "## Datasets \n",
    "\n",
    "The data is provided in `.csv` files.\n",
    "\n",
    "* __day_od.csv__: table of the origins and destinations of the users averaged by the day of the week. The data is provided with details of the month, type of user (resident in Padova/Italian visitor/foreign visitor), country of provenance, together with the province and comune of the user (if available).\n",
    "* __distinct_users_day.csv__: table of the number of distinct users by origin. The data is provided with details of the month, type of user (resident in Padova/Italian visitor/foreign visitor), country of provenance, together with the province and comune of the user (if available).\n",
    "\n",
    "The information is stored in the fields according to the following scheme: \n",
    "\n",
    "- __MONTH__: month analyzed\n",
    "- __DOW__: day analyzed\n",
    "- __ORIGIN__: users' origin area (do not consider this field)\n",
    "- __DESTINATION__: users' destination area (do not consider this field)\n",
    "- __CUST_CLASS__: user type (resident / Italian visitor / foreigner visitor)\n",
    "- __COD_COUNTRY__: users' country code (e.g. 222=Italy)\n",
    "- __COD_PRO__: users' province code (e.g. 12=Varese) \n",
    "- __PRO_COM__: users' comune code (e.g. 12026=Busto Arsizio)\n",
    "- __FLOW__: number of movements for given date-time (with a minimum of 30 users)\n",
    "- __VISITORS__: overall number of users \n",
    "\n",
    "Together with the data files, three lookup-tables are provided to allow matching the Italian institute of STATistics (ISTAT) country, province and comune codes to the actual names.\n",
    "\n",
    "* __codici_istat_comune.csv__: lookup file containing the mapping between _comune_ ISTAT code-names\n",
    "* __codici_istat_provincia.csv__: lookup file containing the mapping between _province_ ISTAT code-names\n",
    "* __codici_nazioni.csv__: lookup file containing mapping the _country_ code to its name\n",
    "\n",
    "Additional information, useful for the study of the flow of users, as the number of inhabitants of each province and the distance between Padova and all other Italian provinces can be extracted based on the data collected by the ISTAT:\n",
    "\n",
    "   - English: https://www.istat.it/en/analysis-and-products/databases, Italian: https://www.istat.it/it/dati-analisi-e-prodotti/banche-dati\n",
    "   \n",
    "   - English/Italian: https://www.istat.it/en/archive/157423, Italian: https://www.istat.it/it/archivio/157423\n",
    "   \n",
    "   - `.zip` package containing the distances between comuni in Veneto region: http://www.istat.it/storage/cartografia/matrici_distanze/Veneto.zip\n",
    "\n",
    "If deemed useful, the open repository [https://github.com/openpolis/geojson-italy](https://github.com/openpolis/geojson-italy) contains a `.json` file with the geographical coordinates of the provences and comuni of Italy.\n",
    "\n",
    "\n",
    "## Assignments\n",
    "\n",
    "1. Data preparation: the csv files are originated from different sources, hence resulting in differences in the encoding and end-of-lines that have to be taken into account in the data preparation phase. Make sure each .csv file is properly interpreted.\n",
    "\n",
    "   1.1 Ranking of visitors from foreign countries: based on the number of total visitors per each country, create a ranked plot of the first 20 countries with the most visitors\n",
    "   \n",
    "   1.2 Ranking of Italian visitors by province, weighted by the number of inhabitants: based on the number of total visitors per Italian province, create a ranked plot of the first 20 provinces with the most visitors taking into account the number of inhabitants.\n",
    "\n",
    "\n",
    "2. Study of the visitors' fluxes: you are asked to provide indications on how to invest resources to improve the mobility towards Padova. Consider the three main directions of visitors and commuters getting to Padova through the main highways (from south, A13 towards Bologna-Roma; from west, A4 towards Milano-Torino; from north-east, A4 towards Venice-Trieste). Evaluate which of the three directions has to be prioritized.\n",
    "\n",
    "   2.1 Consider a simplified case involving only the mid-range mobility, based on the number of visitors/commuters from the nearby regions only\n",
    "   \n",
    "   2.2 Consider the provinces located on the three directions that are mostly contributing to the flow of weekend visitors and working daily commuters by performing a more detailed study of the fluxes based on the day of the week. Use the data available to provide what you believe is the best possible answer.\n",
    "\n",
    "\n",
    "3. Plot the distribution of the number of visitors by the distance of the province of origin. Determine which kind of function should be used to describe the distribution.\n",
    "\n",
    "   3.1 Assuming an analytic form can be used to describe the trend, create a regression or a fit to estimate the expected number of visitors by the distance of the province of origin and the corresponding uncertainties. Illustrate the difference between the resulting regression with respect to the numbers provided by the Vodafone monitoring, and highlight the five most striking discrepancies from the expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet #Used to detect the encoding of the CSV files\n",
    "import codecs  #Used to read the CSV UTF-16\n",
    "import io      #Used to write the CSV ISO-8859-1\n",
    "import pandas as pd #Used to store data into dataframes\n",
    "import matplotlib.pyplot as plt #Used to represent data\n",
    "import numpy as np #Used to rename the column of dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of csv files\n",
    "filename_codici_istat_comuni = \"data\\codici_istat_comune.csv\"\n",
    "filename_codici_istat_provincia = \"data\\codici_istat_provincia.csv\"\n",
    "filename_codici_nazioni = \"data\\codici_nazioni.csv\"\n",
    "filename_day_od = \"data\\day_od.csv\"\n",
    "filename_distinct_user_day = \"data\\distinct_users_day.csv\"\n",
    "filename_distance_to_pd = \"data\\R05_PD.csv\"\n",
    "filename_distance_to_pd_txt = \"data\\R05_PD.txt\"\n",
    "\n",
    "#Creating a list to boost performances of the loops\n",
    "filenames = [filename_codici_istat_comuni, filename_codici_istat_provincia, filename_codici_nazioni,\n",
    "             filename_day_od, filename_distinct_user_day, filename_distance_to_pd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Function which returns the encoding of each csv file\n",
    "def check_encoding(file):\n",
    "    #Read the file\n",
    "    with open(file, 'rb') as f:\n",
    "        #Detect the encoding\n",
    "        result = chardet.detect(f.read())\n",
    "\n",
    "    #Return a list of the encodings\n",
    "    return result['encoding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Function wich converts the UTF-16 encoded files into ISO-8859-1 encoded files\n",
    "def encoding_converter(files):\n",
    "    for file in files:\n",
    "        #Saving the encoding of each file\n",
    "        encodings = check_encoding(file)\n",
    "\n",
    "        # If the encoding is different to ISO-8859-1 it has to be converted\n",
    "        if encodings == 'ascii' :\n",
    "            # Open the file and saving the content\n",
    "            with codecs.open(file, 'r', 'ascii') as f:\n",
    "                data = f.read()\n",
    "\n",
    "            # Overwrite the file with a new encoding\n",
    "            with io.open(file, 'w', encoding='utf-8') as f:\n",
    "                f.write(data)\n",
    "\n",
    "        if encodings == 'utf-16':\n",
    "            # Open the file and saving the content\n",
    "            with codecs.open(file, 'r', 'utf-16') as f:\n",
    "                data = f.read()\n",
    "\n",
    "            # Overwrite the file with a new encoding\n",
    "            with io.open(file, 'w', encoding='latin1') as f:\n",
    "                f.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Ranking of visitors from foreign countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Ranking of visitors from Italy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Study of the visitors' fluxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Praticamente bisogna controllare tutti i caselli autostradali delle 3 autostrade\n",
    "Sommare tutti i flows attraverso i comuni delle 3 autostrade\n",
    "Potrei plottare in base al casello, per ogni autostrada\n",
    "I codici delle persone intendono l'origine delle persone\n",
    "In realtà è una semplificazione perchè immagino che tutti quelli del nordest abbiano preso l'autostrada A4. Farò così:\n",
    "Suddivido in 4 zone: \n",
    "1. NordEst (A4 To-Mi) ad Est di Padova\n",
    "2. NordOvest (A4 Ts-Ve)\n",
    "3. La parte sinistra dell'Italia prende l'A1 che poi diventa A13\n",
    "4. La parte destra dell'Italia prende direttamente l'A13 (Emilia Marche Abruzzo Molise Puglia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toll_booths(highway = \"\"):   \n",
    "    data_tolls = None\n",
    "    if highway == \"A4To-Mi\" :\n",
    "        data_tolls = pd.DataFrame({'COMUNE':[\"Torino\", \"Borgo d'Ale\", \"Santhià\", \"Carisio\", \"Balocco\", \"Greggio\", \"Biandrate\", \"Novara\",\n",
    "            \"Mesero\", \"Arluno\", \"Rho\", \"Milano\", \"Monza\", \"Agrate\", \"Cavenago\", \"Trezzo\", \"Capriate\", \"Dalmine\",\n",
    "            \"Bergamo\", \"Seriate\", \"Grumello\", \"Ponte Oglio\", \"Palazzolo\", \"Rovato\", \"Ospitaletto\", \"Castegnato\",\n",
    "            \"Brescia\", \"Desenzano\", \"Sirmione\", \"Peschiera\", \"Sommacampagna\", \"Verona\", \"Soave\", \"Montebello\",\n",
    "            \"Montecchio\", \"Vicenza\", \"Grisignano\"]})\n",
    "        \n",
    "    elif highway == \"A4Ts-Ve\":\n",
    "        data_tolls = pd.DataFrame({\"COMUNE\": [\"Trieste\", \"Redipuglia\", \"Palmanova\", \"San Giorgio di Nogaro\", \n",
    "                                              \"Latisana\", \"San Stino di Livenza\", \"Cessalto\", \"San Donà di Piave\", \n",
    "                                              \"Meolo\", \"Preganziol\", \"Martellago\", \"Spinea\"]})\n",
    "\n",
    "    elif highway == \"A1Ro-Bo\":\n",
    "        data_tolls = pd.DataFrame({'COMUNE':[\"Sasso Marconi\", \"Rioveggio\", \"Pian del Voglio\", \"Roncobilaccio\", \"Barberino di Mugello\",\n",
    "            \"Calenzano\", \"Firenze\", \"Incisa\", \"Valdarno\", \"Arezzo\", \"Monte San Savino\", \"Valdichiana\", \"Chiusi\",\n",
    "            \"Fabro\", \"Orvieto\", \"Attigliano\", \"Orte\", \"Magliano Sabina\", \"Ponzano Romano\", \"Guidonia Montecelio\",\n",
    "            \"Valmontone\", \"Colleferro\", \"Anagni\"]})\n",
    "\n",
    "    else: # highway == \"A13Bo-Pd\"\n",
    "        #Altedo è fraz di Bologna\n",
    "        data_tolls = pd.DataFrame({'COMUNE':[\"Bologna\", \"Ferrara\", \"Occhiobello\", \"Rovigo\", \"Boara Pisani\", \"Monselice\"]})\n",
    "        \n",
    "    return data_tolls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def possible_highway(data):\n",
    "    # Ignore Padova because they do not take the highway\n",
    "    data_no = data.loc[(data['REGION'] == \"Lombardia\") | (data['REGION'] == \"Trentino-Alto Adige\") | (data['REGION'] == \"Valle d'Aosta\")\n",
    "                       | (data['REGION'] == \"Piemonte\") | (data['REGION'] == \"Liguria\")\n",
    "                       | (data['COD_PRO'] == 24) | (data['COD_PRO'] == 23)]  # Vicenza and Verona\n",
    "    \n",
    "    data_ne = data.loc[(data['REGION'] == \"Friuli-Venezia Giulia\") |\n",
    "                        (data['COD_PRO'] == 25) | (data['COD_PRO'] == 26) | (data['COD_PRO'] == 27)]  # Belluno, Treviso, and Venezia\n",
    "    \n",
    "    data_so = data.loc[(data['REGION'] == \"Emilia Romagna\") | (data['REGION'] == \"Marche\") | (data['REGION'] == \"Abruzzo\")\n",
    "                       | (data['REGION'] == \"Molise\") | (data['REGION'] == \"Puglia\") \n",
    "                       | (data['COD_PRO'] == 29)]  # Rovigo\n",
    "    \n",
    "    data_se = data.loc[(data['REGION'] == \"Toscana\") | (data['REGION'] == \"Umbria\") | (data['REGION'] == \"Lazio\")\n",
    "                       | (data['REGION'] == \"Campania\") | (data['REGION'] == \"Basilicata\") | (data['REGION'] == \"Calabria\")]\n",
    "                       \n",
    "    data_groups  = [data_no, data_ne, data_so, data_se]\n",
    "    list_highways = [\"A4To-Mi\", \"A4Ts-Ve\", \"A13Bo-Pd\", \"A1Ro-Bo\"]\n",
    "\n",
    "    for x in range(0, len(data_groups)):\n",
    "        # CALCULATION OF FLOWS OF MUNICIPALITIES WITH TOLLBOOTH\n",
    "        comuni_toll = toll_booths(list_highways[x])\n",
    "\n",
    "        # Add a new column \"toll booth number\" to the DataFrame\n",
    "        comuni_toll['N_TOLL'] = range(1, len(comuni_toll) + 1)\n",
    "\n",
    "        # Check for matches, if found, calculate the sum\n",
    "        data_comuni_toll = pd.merge(data_groups[x], comuni_toll, on=\"COMUNE\")\n",
    "        # CALCULATION OF FLOWS OF PROVINCES WITH TOLLBOOTH\n",
    "        # Otherwise, check the province\n",
    "        comuni_codes = pd.read_csv(filename_codici_istat_comuni, encoding='latin1', usecols=['COMUNE', 'PRO_COM', 'COD_PRO', 'COMUNE_CAPOLUOGO'])\n",
    "        comuni_toll = pd.merge(comuni_toll, comuni_codes, on=\"COMUNE\")\n",
    "        # Subtraction between two dataframes\n",
    "        data_without_comuni_toll = data_groups[x].merge(data_comuni_toll, how='outer', indicator=True).query('_merge == \"left_only\"').drop('_merge', axis=1)\n",
    "        # Take the provinces of the municipalities with toll booths\n",
    "        data_province_toll = data_without_comuni_toll.drop(['COMUNE','PRO_COM'], axis= 1)\n",
    "        data_province_toll = pd.merge(data_province_toll, comuni_toll, on='COD_PRO')\n",
    "        data_province_toll = data_province_toll.groupby(['COD_PRO'])[['VISITORS']].sum()\n",
    "\n",
    "        # Merge the two DataFrames based on the 'COD_PRO' column\n",
    "        data_com_pro_toll = pd.merge(data_province_toll, data_comuni_toll, suffixes=('_df1', '_df2'), on='COD_PRO', how='outer')\n",
    "\n",
    "        # Sum the values of the 'visitors' column of the two DataFrames\n",
    "        data_com_pro_toll['VISITORS'] = data_com_pro_toll['VISITORS_df1'].fillna(0) + data_com_pro_toll['VISITORS_df2'].fillna(0)\n",
    "        data_com_pro_toll = data_com_pro_toll.drop(['VISITORS_df1','VISITORS_df2'], axis= 1)\n",
    "\n",
    "        data_com_pro_toll = data_com_pro_toll.dropna()\n",
    "\n",
    "        # CALCULATION OF FLOWS FOR USERS WHO DO NOT HAVE A TOLLBOOTH IN THE PROVINCE -> UNIFORMLY DISTRIBUTE AMONG THE TOLL BOOTHS\n",
    "        # Take all the provinces of data_groups[x] -> Remove those with the toll booth\n",
    "        province = set(data_groups[x]['COD_PRO'].unique())\n",
    "        province_with_toll = set(data_com_pro_toll['COD_PRO'].unique())\n",
    "        province_without_toll = pd.DataFrame({'COD_PRO': list(province - province_with_toll)})\n",
    "        \n",
    "        data_region_toll = pd.merge(data_groups[x], province_without_toll, on=\"COD_PRO\")\n",
    "        \n",
    "        # Sum the flow of the provinces\n",
    "        data_region_toll = data_region_toll.groupby('COD_PRO')[['VISITORS']].sum()\n",
    "        # Uniformize\n",
    "        if len(data_com_pro_toll) == 0:\n",
    "            data_groups.pop(x)\n",
    "        else:\n",
    "            flow = (data_region_toll['VISITORS'].sum() / len(data_com_pro_toll)).round()\n",
    "            # Sum\n",
    "            data_com_pro_toll['VISITORS'] = data_com_pro_toll['VISITORS'] + flow\n",
    "            \n",
    "            data_groups[x] = data_com_pro_toll\n",
    "    \n",
    "    return data_groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_region(data):\n",
    "    # Dictionary mapping region names to their respective codes\n",
    "    regioni = {\n",
    "        'Abruzzo': 13,\n",
    "        'Basilicata': 17,\n",
    "        'Calabria': 18,\n",
    "        'Campania': 15,\n",
    "        'Emilia Romagna': 8,\n",
    "        'Friuli-Venezia Giulia': 6,\n",
    "        'Lazio': 12,\n",
    "        'Liguria': 7,\n",
    "        'Lombardia': 3,\n",
    "        'Marche': 11,\n",
    "        'Molise': 14,\n",
    "        'Piemonte': 1,\n",
    "        'Puglia': 16,\n",
    "        'Sardegna': 20,\n",
    "        'Sicilia': 19,\n",
    "        'Toscana': 9,\n",
    "        'Trentino-Alto Adige': 4,\n",
    "        'Umbria': 10,\n",
    "        'Valle d\\'Aosta': 2,\n",
    "        'Veneto': 5\n",
    "    }\n",
    "    \n",
    "    # Initialize a list to store the names of the regions\n",
    "    region_names = []\n",
    "    \n",
    "    # Iterate through each row of the DataFrame\n",
    "    for index, row in data.iterrows():\n",
    "        # Get the region code from the 'COD_REG' column\n",
    "        cod_reg = row['COD_REG']\n",
    "        \n",
    "        # Search for the region name corresponding to the code in the 'COD_REG' column\n",
    "        for region_name, region_code in regioni.items():\n",
    "            if cod_reg == region_code:\n",
    "                # Add the region name to the list\n",
    "                region_names.append(region_name)\n",
    "                break\n",
    "        else:\n",
    "            # If the region code is not present in the 'regioni' dictionary, add None\n",
    "            region_names.append(None)\n",
    "    \n",
    "    # Add the list of region names as a new 'REGION' column in the DataFrame\n",
    "    data['REGION'] = region_names\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(data, highway=True):\n",
    "    # Extract the relevant data from the DataFrame\n",
    "    # Since the Italian state declared the independence of the province of Monza only in 2004, its code is messy,\n",
    "    # so we manually reorder it after sorting by 'N_TOLL'\n",
    "    if highway:\n",
    "        data = data.sort_values(by='N_TOLL')\n",
    "        comuni = data['COMUNE']\n",
    "    else:\n",
    "        comuni = data['PROVINCIA']\n",
    "    visitatori = data['VISITORS']\n",
    "    visitatori_cumulative = data['VISITORS'].cumsum()\n",
    "\n",
    "    # Create the bar plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(comuni, visitatori, color='skyblue', alpha=0.5)\n",
    "    plt.plot(comuni, visitatori_cumulative, color='red')  # Plot cumulative visitors\n",
    "\n",
    "    # Add titles and labels\n",
    "    plt.title('Number of visitors per municipality')\n",
    "    plt.xlabel('Municipality')\n",
    "    plt.ylabel('Number of visitors')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    # Set the scale to avoid the exponential notation for large numbers on the y-axis\n",
    "    plt.ticklabel_format(style='plain', axis='y')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visitors_fluxes(dow_study=False, mid_range=False, highway=True):\n",
    "    # Consider the ISTAT province codes\n",
    "    comuni_codes = pd.read_csv(filename_codici_istat_comuni, encoding='latin1', usecols=['COMUNE', 'PRO_COM', 'COD_PRO'])\n",
    "    provinces_codes = pd.read_csv(filename_codici_istat_provincia, encoding='latin1', usecols=['COD_PRO', 'COD_REG'])\n",
    "\n",
    "    codes = pd.merge(comuni_codes, provinces_codes, on='COD_PRO')\n",
    "\n",
    "    if highway:\n",
    "        if dow_study:\n",
    "            customers_data = pd.read_csv(filename_day_od, encoding='latin1', usecols=['FLOW', 'PRO_COM', 'DOW'])\n",
    "            customers_data.rename(columns={'FLOW': 'VISITORS'}, inplace=True)\n",
    "        else:\n",
    "            # Now take the data of the customers and see what's their origin and their destination\n",
    "            customers_data = pd.read_csv(filename_distinct_user_day, encoding='latin1', usecols=['VISITORS', 'PRO_COM'])\n",
    "    else:\n",
    "        # If not highway mode, handle urban roads\n",
    "        customers_data = pd.read_csv(filename_distinct_user_day, encoding='latin1', usecols=['VISITORS', 'PRO_COM', 'COD_PRO'])\n",
    "\n",
    "    customers_data.dropna(axis=0, inplace=True)\n",
    "\n",
    "    # Add region information based on the province codes\n",
    "    users = add_region(codes)\n",
    "\n",
    "    if mid_range:\n",
    "        # Filtering regions based on specified criteria\n",
    "        maskLombardia = (users['REGION'] == 'Lombardia')\n",
    "        maskTrentino = (users['REGION'] == 'Trentino-Alto Adige')\n",
    "        maskFriuli = (users['REGION'] == 'Friuli-Venezia Giulia')\n",
    "        maskEmilia = (users['REGION'] == 'Emilia Romagna')\n",
    "    \n",
    "    if highway:\n",
    "        if mid_range:\n",
    "            # Filter users based on specified mid-range regions\n",
    "            users = users.loc[maskLombardia | maskTrentino | maskFriuli | maskEmilia]\n",
    "    \n",
    "        # Join between customers_data and province_codes to filter the province_codes we're interested in\n",
    "        if dow_study:\n",
    "            customers_data = customers_data.groupby(['PRO_COM','DOW'])[['VISITORS']].sum().reset_index()\n",
    "            data = pd.merge(customers_data, users, on='PRO_COM')\n",
    "        else: \n",
    "            customers_data = customers_data.groupby(['PRO_COM'])[['VISITORS']].sum().reset_index()\n",
    "            data = pd.merge(customers_data, users, on='PRO_COM')\n",
    "\n",
    "        # Obtain possible highways and their traffic data\n",
    "        highways = possible_highway(data)\n",
    "\n",
    "        # Plot the traffic flow for each highway region\n",
    "        for i, region_group in enumerate(highways):\n",
    "            highways[i] = region_group.drop(['COD_PRO', 'PRO_COM', 'COD_REG', 'REGION'], axis=1)\n",
    "            if not dow_study:\n",
    "                plotter(region_group)\n",
    "            else:\n",
    "                #Da sistemare\n",
    "                data_weekend = region_group.loc[(region_group['DOW'] == \"Domenica\") | (region_group['DOW'] == \"Sabato\")]\n",
    "                data_workingdays = region_group.loc[(region_group['DOW'] == \"Lunedì\") | (region_group['DOW'] == \"Martedì\")\n",
    "                                                    | (region_group['DOW'] == \"Mercoledì\") | (region_group['DOW'] == \"Giovedì\")\n",
    "                                                    | (region_group['DOW'] == \"Venerdì\")]\n",
    "                \n",
    "                data_weekend = data_weekend.groupby(['PRO_COM','N_TOLL','COMUNE'])[['VISITORS']].sum().reset_index()\n",
    "                data_workingdays = data_weekend.groupby(['PRO_COM','N_TOLL','COMUNE'])[['VISITORS']].sum().reset_index()\n",
    "                # plotter(data_weekend)\n",
    "                print(data_workingdays)\n",
    "    else:\n",
    "        # Divide provinces into 4 groups: North, East, South, West, based on the region's position\n",
    "        users_ovest = users.loc[maskLombardia]\n",
    "        users_nord = users.loc[maskTrentino]\n",
    "        users_est = users.loc[maskFriuli]\n",
    "        users_sud = users.loc[maskEmilia]\n",
    "\n",
    "        group_list = [users_ovest, users_nord, users_est, users_sud]\n",
    "\n",
    "        customers_data = customers_data.drop(['PRO_COM'], axis=1)\n",
    "        # Join between customers_data and province_codes to filter the province_codes we're interested in\n",
    "        customers_data = customers_data.groupby('COD_PRO').sum()\n",
    "\n",
    "        provinces = pd.read_csv(filename_codici_istat_provincia, encoding='latin1', usecols=['COD_PRO', 'PROVINCIA'])\n",
    "        customers_data = pd.merge(customers_data, provinces, on='COD_PRO')\n",
    "\n",
    "        for users_group in group_list:\n",
    "            data = pd.merge(customers_data, users_group, on='COD_PRO')\n",
    "            data = data.groupby('PROVINCIA')[['VISITORS']].sum().reset_index()\n",
    "\n",
    "            plotter(data, highway=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PRO_COM  N_TOLL   COMUNE  VISITORS\n",
      "0   1272.0     1.0   Torino    3068.0\n",
      "1   3106.0     8.0   Novara     686.0\n",
      "2  15146.0    12.0   Milano   27082.0\n",
      "3  17029.0    27.0  Brescia    3440.0\n",
      "4  23081.0    33.0    Soave   27940.0\n",
      "5  23091.0    32.0   Verona   64312.0\n",
      "6  24116.0    36.0  Vicenza  211808.0\n",
      "   PRO_COM  N_TOLL      COMUNE   VISITORS\n",
      "0  26063.0    10.0  Preganziol   258636.0\n",
      "1  27021.0    11.0  Martellago  4597021.0\n",
      "2  27038.0    12.0      Spinea  4597669.0\n",
      "3  32006.0     1.0     Trieste     2461.0\n",
      "   PRO_COM  N_TOLL   COMUNE  VISITORS\n",
      "0  29041.0       4   Rovigo   78194.0\n",
      "1  37006.0       1  Bologna    5927.0\n",
      "2  38008.0       2  Ferrara    3249.0\n",
      "   PRO_COM  N_TOLL   COMUNE  VISITORS\n",
      "0  48017.0     7.0  Firenze    3438.0\n"
     ]
    }
   ],
   "source": [
    "visitors_fluxes(dow_study=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visitors_fluxes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Mid-range mobility through highways\n",
    "Mobility to/from nearby regions, which are Lombardia, Trentino Alto Adige, Friuli Venezia Giulia, Emilia Romagna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visitors_fluxes(mid_range=True) #Through Highways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visitors_fluxes(mid_range=True, highway=False) #Through Urban roads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Week flow\n",
    "\n",
    "Consider the provinces located on the three directions that are mostly contributing to the flow of weekend visitors and working daily commuters by performing a more detailed study of the fluxes based on the day of the week. Use the data available to provide what you believe is the best possible answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visitors_fluxes(dow_study=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3\n",
    "Plot the distribution of the number of visitors by the distance of the province of origin. Determine which kind of function should be used to describe the distribution.\n",
    "\n",
    "è scritto visitor quindi tolgo i non visitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_visitors_distance(province_data):\n",
    "    distances_data = pd.read_csv(province_data, sep=\"\\t\", encoding=\"UTF-8\", usecols=['DEST_PROCOM', 'KM_TOT'])\n",
    "\n",
    "    distances_data.rename(columns={'DEST_PROCOM': 'PRO_COM'}, inplace=True)\n",
    "\n",
    "    distances_data['KM_TOT'] = distances_data['KM_TOT'].str.replace(',','.')\n",
    "    distances_data['KM_TOT'] = pd.to_numeric(distances_data['KM_TOT'])\n",
    "\n",
    "    distances_data = distances_data.groupby(['PRO_COM'])[['KM_TOT']].mean()\n",
    "\n",
    "    customers_data = pd.read_csv(filename_distinct_user_day, encoding=\"latin1\", usecols=['PRO_COM','VISITORS','CUST_CLASS'])\n",
    "\n",
    "    # Escludo Padova\n",
    "    customers_data['PRO_COM'] = np.where(customers_data['PRO_COM'] == 28060.0 , np.nan,\n",
    "                                np.where(customers_data['CUST_CLASS'] != 'visitor', np.nan, customers_data['PRO_COM']))\n",
    "    customers_data.dropna(axis=0, inplace=True)\n",
    "\n",
    "    # Elimina la colonna \"CUST_CLASS\"\n",
    "    customers_data = customers_data.drop('CUST_CLASS', axis=1)\n",
    "\n",
    "    customers_data = customers_data.groupby(['PRO_COM'])[['VISITORS']].sum()\n",
    "\n",
    "    data = pd.merge(distances_data, customers_data, on='PRO_COM')\n",
    "\n",
    "    data = data.sort_values(['VISITORS'], ascending=False)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_visitors_distance(filename_distance_to_pd_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_visitors_distance(data, zoom=1):\n",
    "    # zoom serve a raggruppare le distanze (50 --> raggruppo le distanze ogni 50 km)\n",
    "    # calcolo il range di raggruppamento\n",
    "    period = int(data['KM_TOT'].max() / zoom)\n",
    "    somma_visitatori = []\n",
    "    for n_group in range(1,period+1):\n",
    "        data_grouped = data[data['KM_TOT'].between((n_group-1)*zoom +1 , n_group * zoom)]\n",
    "        somma_visitatori.append(data_grouped['VISITORS'].sum())\n",
    "    \n",
    "    data_to_plot = pd.DataFrame({'DISTANCE_TO_PD': [km * zoom for km in range(0, period)], 'VISITORS': somma_visitatori})\n",
    "    # Impostare l'indice dopo la creazione del DataFrame\n",
    "    data_to_plot.set_index('DISTANCE_TO_PD', inplace=True)\n",
    "\n",
    "    # data_to_plot['KM_TOT'] = np.where(data_to_plot['KM_TOT'] == 0 , np.nan, data_to_plot['KM_TOT'])\n",
    "    # data_to_plot.dropna(axis=0, inplace=True)\n",
    "\n",
    "    # data_nearby = data.loc[(data['KM_TOT'] < 50)]\n",
    "\n",
    "    # data_far = data.loc[~data.index.isin(data_nearby.index)]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(data_to_plot['VISITORS'], color='blue', alpha=0.7, bins=20)\n",
    "    plt.xlabel('Distance from Padova (KM)')\n",
    "    plt.ylabel('Number of Visitors')\n",
    "    plt.title('Histogram of Distance from Padova')\n",
    "    plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "    plt.show()\n",
    "    #Da riguardare il grafico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_visitors_distance(data_visitors_distance(filename_distance_to_pd_txt), 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 \n",
    "Assuming an analytic form can be used to describe the trend, create a regression or a fit to estimate the expected number of visitors by the distance of the province of origin and the corresponding uncertainties. Illustrate the difference between the resulting regression with respect to the numbers provided by the Vodafone monitoring, and highlight the five most striking discrepancies from the expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Build linear regression model using TV and Radio as predictors\n",
    "# Split data into predictors X and output Y\n",
    "X = np.array(data['KM_TOT']).reshape(-1,1)\n",
    "y = data['VISITORS']\n",
    "\n",
    "# Initialise and fit model\n",
    "lm = LinearRegression()\n",
    "model = lm.fit(X, y)\n",
    "# plot for residual error\n",
    " \n",
    "   \n",
    "plt.scatter(X,y,color='red')\n",
    "plt.plot(X,model.predict(X),color='green')\n",
    "plt.title('Simple Linear Regression')\n",
    "plt.xlabel('Position Level')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
