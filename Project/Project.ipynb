{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Vodafone users' fluxes\n",
    "\n",
    "The study of the flux of people inside urban areas is of paramount importance to achieve an optimal understanding of emerging critical issues in the local mobility, and to explore areas of potential improvements in the infrastructures and local transports.\n",
    "\n",
    "The mobility of users within and toward Padova has been monitored using the data provided by the Vodafone mobile carrier, which provides the information based on the users' connections to the network cells.\n",
    "The data provided by the carrier encompasses the monitoring of the users connected to the Vodafone network in Padova in a four-month period from February to May of 2018.\n",
    "\n",
    "To provide statistical insights on the number and the flow of users, the data is aggregated based on the origin and movements of the users by averaging the number of connections during the time of the monitoring.\n",
    "\n",
    "To further avoid privacy violation issues, all observations with less than 30 units (e.g. day-areas for which $<$ 30 users have contributed) have been discarded and/or merged into dedicated categories (indicated with \"altro\", or \"other\").\n",
    "\n",
    "\n",
    "## Datasets \n",
    "\n",
    "The data is provided in `.csv` files.\n",
    "\n",
    "* __day_od.csv__: table of the origins and destinations of the users averaged by the day of the week. The data is provided with details of the month, type of user (resident in Padova/Italian visitor/foreign visitor), country of provenance, together with the province and comune of the user (if available).\n",
    "* __distinct_users_day.csv__: table of the number of distinct users by origin. The data is provided with details of the month, type of user (resident in Padova/Italian visitor/foreign visitor), country of provenance, together with the province and comune of the user (if available).\n",
    "\n",
    "The information is stored in the fields according to the following scheme: \n",
    "\n",
    "- __MONTH__: month analyzed\n",
    "- __DOW__: day analyzed\n",
    "- __ORIGIN__: users' origin area (do not consider this field)\n",
    "- __DESTINATION__: users' destination area (do not consider this field)\n",
    "- __CUST_CLASS__: user type (resident / Italian visitor / foreigner visitor)\n",
    "- __COD_COUNTRY__: users' country code (e.g. 222=Italy)\n",
    "- __COD_PRO__: users' province code (e.g. 12=Varese) \n",
    "- __PRO_COM__: users' comune code (e.g. 12026=Busto Arsizio)\n",
    "- __FLOW__: number of movements for given date-time (with a minimum of 30 users)\n",
    "- __VISITORS__: overall number of users \n",
    "\n",
    "Together with the data files, three lookup-tables are provided to allow matching the Italian institute of STATistics (ISTAT) country, province and comune codes to the actual names.\n",
    "\n",
    "* __codici_istat_comune.csv__: lookup file containing the mapping between _comune_ ISTAT code-names\n",
    "* __codici_istat_provincia.csv__: lookup file containing the mapping between _province_ ISTAT code-names\n",
    "* __codici_nazioni.csv__: lookup file containing mapping the _country_ code to its name\n",
    "\n",
    "Additional information, useful for the study of the flow of users, as the number of inhabitants of each province and the distance between Padova and all other Italian provinces can be extracted based on the data collected by the ISTAT:\n",
    "\n",
    "   - English: https://www.istat.it/en/analysis-and-products/databases, Italian: https://www.istat.it/it/dati-analisi-e-prodotti/banche-dati\n",
    "   \n",
    "   - English/Italian: https://www.istat.it/en/archive/157423, Italian: https://www.istat.it/it/archivio/157423\n",
    "   \n",
    "   - `.zip` package containing the distances between comuni in Veneto region: http://www.istat.it/storage/cartografia/matrici_distanze/Veneto.zip\n",
    "\n",
    "If deemed useful, the open repository [https://github.com/openpolis/geojson-italy](https://github.com/openpolis/geojson-italy) contains a `.json` file with the geographical coordinates of the provences and comuni of Italy.\n",
    "\n",
    "\n",
    "## Assignments\n",
    "\n",
    "1. Data preparation: the csv files are originated from different sources, hence resulting in differences in the encoding and end-of-lines that have to be taken into account in the data preparation phase. Make sure each .csv file is properly interpreted.\n",
    "\n",
    "   1.1 Ranking of visitors from foreign countries: based on the number of total visitors per each country, create a ranked plot of the first 20 countries with the most visitors\n",
    "   \n",
    "   1.2 Ranking of Italian visitors by province, weighted by the number of inhabitants: based on the number of total visitors per Italian province, create a ranked plot of the first 20 provinces with the most visitors taking into account the number of inhabitants.\n",
    "\n",
    "\n",
    "2. Study of the visitors' fluxes: you are asked to provide indications on how to invest resources to improve the mobility towards Padova. Consider the three main directions of visitors and commuters getting to Padova through the main highways (from south, A13 towards Bologna-Roma; from west, A4 towards Milano-Torino; from north-east, A4 towards Venice-Trieste). Evaluate which of the three directions has to be prioritized.\n",
    "\n",
    "   2.1 Consider a simplified case involving only the mid-range mobility, based on the number of visitors/commuters from the nearby regions only\n",
    "   \n",
    "   2.2 Consider the provinces located on the three directions that are mostly contributing to the flow of weekend visitors and working daily commuters by performing a more detailed study of the fluxes based on the day of the week. Use the data available to provide what you believe is the best possible answer.\n",
    "\n",
    "\n",
    "3. Plot the distribution of the number of visitors by the distance of the province of origin. Determine which kind of function should be used to describe the distribution.\n",
    "\n",
    "   3.1 Assuming an analytic form can be used to describe the trend, create a regression or a fit to estimate the expected number of visitors by the distance of the province of origin and the corresponding uncertainties. Illustrate the difference between the resulting regression with respect to the numbers provided by the Vodafone monitoring, and highlight the five most striking discrepancies from the expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet #Used to detect the encoding of the CSV files\n",
    "import codecs  #Used to read the CSV UTF-16\n",
    "import io      #Used to write the CSV ISO-8859-1\n",
    "import pandas as pd #Used to store data into dataframes\n",
    "import matplotlib.pyplot as plt #Used to represent data\n",
    "import numpy as np #Used to rename the column of dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of csv files\n",
    "filename_codici_istat_comuni = \"data\\codici_istat_comune.csv\"\n",
    "filename_codici_istat_provincia = \"data\\codici_istat_provincia.csv\"\n",
    "filename_codici_nazioni = \"data\\codici_nazioni.csv\"\n",
    "filename_day_od = \"data\\day_od.csv\"\n",
    "filename_distinct_user_day = \"data\\distinct_users_day.csv\"\n",
    "filename_distance_to_pd = \"data\\R05_PD.csv\"\n",
    "filename_distance_to_pd_txt = \"data\\R05_PD.txt\"\n",
    "\n",
    "#Creating a list to boost performances of the loops\n",
    "filenames = [filename_codici_istat_comuni, filename_codici_istat_provincia, filename_codici_nazioni,\n",
    "             filename_day_od, filename_distinct_user_day, filename_distance_to_pd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Function which returns the encoding of each csv file\n",
    "def check_encoding(file):\n",
    "    #Read the file\n",
    "    with open(file, 'rb') as f:\n",
    "        #Detect the encoding\n",
    "        result = chardet.detect(f.read())\n",
    "\n",
    "    #Return a list of the encodings\n",
    "    return result['encoding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Function wich converts the UTF-16 encoded files into ISO-8859-1 encoded files\n",
    "def encoding_converter(files):\n",
    "    for file in files:\n",
    "        #Saving the encoding of each file\n",
    "        encodings = check_encoding(file)\n",
    "\n",
    "        # If the encoding is different to ISO-8859-1 it has to be converted\n",
    "        if encodings == 'ascii' :\n",
    "            # Open the file and saving the content\n",
    "            with codecs.open(file, 'r', 'ascii') as f:\n",
    "                data = f.read()\n",
    "\n",
    "            # Overwrite the file with a new encoding\n",
    "            with io.open(file, 'w', encoding='utf-8') as f:\n",
    "                f.write(data)\n",
    "\n",
    "        if encodings == 'utf-16':\n",
    "            # Open the file and saving the content\n",
    "            with codecs.open(file, 'r', 'utf-16') as f:\n",
    "                data = f.read()\n",
    "\n",
    "            # Overwrite the file with a new encoding\n",
    "            with io.open(file, 'w', encoding='latin1') as f:\n",
    "                f.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Ranking of visitors from foreign countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Ranking of visitors from Italy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Study of the visitors' fluxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Praticamente bisogna controllare tutti i caselli autostradali delle 3 autostrade\n",
    "Sommare tutti i flows attraverso i comuni delle 3 autostrade\n",
    "Potrei plottare in base al casello, per ogni autostrada\n",
    "I codici delle persone intendono l'origine delle persone\n",
    "In realtà è una semplificazione perchè immagino che tutti quelli del nordest abbiano preso l'autostrada A4. Farò così:\n",
    "Suddivido in 4 zone: \n",
    "1. NordEst (A4 To-Mi) ad Est di Padova\n",
    "2. NordOvest (A4 Ts-Ve)\n",
    "3. La parte sinistra dell'Italia prende l'A1 che poi diventa A13\n",
    "4. La parte destra dell'Italia prende direttamente l'A13 (Emilia Marche Abruzzo Molise Puglia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toll_booths(highway = \"\"):   \n",
    "    if highway == \"A4To-Mi\" :\n",
    "        data_tolls = pd.DataFrame({'COMUNE':[\"Torino\", \"Borgo d'Ale\", \"Santhià\", \"Carisio\", \"Balocco\", \"Greggio\", \"Biandrate\", \"Novara\",\n",
    "            \"Mesero\", \"Arluno\", \"Rho\", \"Milano\", \"Monza\", \"Agrate\", \"Cavenago\", \"Trezzo\", \"Capriate\", \"Dalmine\",\n",
    "            \"Bergamo\", \"Seriate\", \"Grumello\", \"Ponte Oglio\", \"Palazzolo\", \"Rovato\", \"Ospitaletto\", \"Castegnato\",\n",
    "            \"Brescia\", \"Desenzano\", \"Sirmione\", \"Peschiera\", \"Sommacampagna\", \"Verona\", \"Soave\", \"Montebello\",\n",
    "            \"Montecchio\", \"Vicenza\", \"Grisignano\"]})\n",
    "        \n",
    "    elif highway == \"A4Ts-Ve\":\n",
    "        data_tolls = pd.DataFrame({'COMUNE':[\"Spinea\", \"Martellago\", \"Preganziol\", \"Meolo\", \"San Donà di Piave\", \"Cessalto\",\n",
    "            \"San Stino di Livenza\", \"Latisana\", \"San Giorgio di Nogaro\", \"Palmanova\", \"Redipuglia\", \"Trieste\"]})\n",
    "\n",
    "    elif highway == \"A1Ro-Bo\":\n",
    "        data_tolls = pd.DataFrame({'COMUNE':[\"Sasso Marconi\", \"Rioveggio\", \"Pian del Voglio\", \"Roncobilaccio\", \"Barberino di Mugello\",\n",
    "            \"Calenzano\", \"Firenze\", \"Incisa\", \"Valdarno\", \"Arezzo\", \"Monte San Savino\", \"Valdichiana\", \"Chiusi\",\n",
    "            \"Fabro\", \"Orvieto\", \"Attigliano\", \"Orte\", \"Magliano Sabina\", \"Ponzano Romano\", \"Guidonia Montecelio\",\n",
    "            \"Valmontone\", \"Colleferro\", \"Anagni\"]})\n",
    "\n",
    "    elif highway == \"A13Bo-Pd\":\n",
    "        #Altedo è fraz di Bologna\n",
    "        data_tolls = pd.DataFrame({'COMUNE':[\"Bologna\", \"Ferrara\", \"Occhiobello\", \"Rovigo\", \"Boara Pisani\", \"Monselice\"]})\n",
    "        \n",
    "    return data_tolls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def possible_highway(data):\n",
    "\n",
    "    #Ignoro Padova perchè non prendono l'autostrada\n",
    "    data_no = data.loc[(data['REGION'] == \"Lombardia\") | (data['REGION'] == \"Trentino-Alta Adige\") | (data['REGION'] == \"Valle d'Aosta\")\n",
    "                       | (data['REGION'] == \"Piemonte\") | (data['REGION'] == \"Liguria\")\n",
    "                       | ((data['REGION'] == \"VENETO\") & ((data['COD_PRO'] == 24) | (data['COD_PRO'] == 23)))] #Vicenza e Verona\n",
    "    \n",
    "    data_ne = data.loc[(data['REGION'] == \"Friuli-Venezia Giulia\") | ((data['REGION'] == \"VENETO\") & \n",
    "                        ((data['COD_PRO'] == 25)| (data['COD_PRO'] == 26) | (data['COD_PRO'] == 27)))] #Belluno, Treviso e Venezia\n",
    "    \n",
    "    data_so = data.loc[(data['REGION'] == \"Emilia Romagna\") | (data['REGION'] == \"Marche\") | (data['REGION'] == \"Abruzzo\")\n",
    "                       | (data['REGION'] == \"Molise\") | (data['REGION'] == \"Puglia\") \n",
    "                       | ((data['REGION'] == \"VENETO\") & (data['COD_PRO'] == 29))] #Rovigo\n",
    "    \n",
    "    data_se = data.loc[(data['REGION'] == \"Toscana\") | (data['REGION'] == \"Umbria\") | (data['REGION'] == \"Lazio\")\n",
    "                       | (data['REGION'] == \"Campania\") | (data['REGION'] == \"Basilicata\") | (data['REGION'] == \"Calabria\")]\n",
    "                       \n",
    "    data_groups  = [data_no, data_ne, data_so, data_se]\n",
    "    list_highways = [\"A4To-Mi\", \"A4Ts-Ve\", \"A13Bo-Pd\", \"A1Ro-Bo\"]\n",
    "\n",
    "    '''\n",
    "    for x in range(0,len(data_groups)):\n",
    "        # CALCOLO DEI FLUSSI DEI COMUNI CON CASELLO\n",
    "        comuni_toll = toll_booths(list_highways[x])\n",
    "        #Controllo corrispondenze, se trovo corrispondenze calcolo la somma\n",
    "        data_comuni_toll = pd.merge(data_groups[x], comuni_toll, on=\"COMUNE\")\n",
    "\n",
    "        # CALCOLO DEI FLUSSI DELLE PROVINCE CON CASELLO\n",
    "        #Altrimenti controllo la provincia\n",
    "        comuni_codes = pd.read_csv(filename_codici_istat_comuni, encoding='latin1', usecols=['COMUNE', 'PRO_COM', 'COD_PRO', 'COMUNE_CAPOLUOGO'])\n",
    "        comuni_toll = pd.merge(comuni_toll, comuni_codes, on=\"COMUNE\")\n",
    "        # Subtraction between two dataframe\n",
    "        data_without_comuni_toll = data_groups[x].merge(data_comuni_toll, how='outer', indicator=True).query('_merge == \"left_only\"').drop('_merge', axis=1)\n",
    "        # Prendo le province dei comuni con i caselli\n",
    "        data_province_toll = data_without_comuni_toll.drop(['COMUNE','PRO_COM'], axis= 1)\n",
    "        data_province_toll = pd.merge(data_province_toll, comuni_toll, on='COD_PRO')\n",
    "        data_province_toll = data_province_toll.groupby(['COD_PRO'])[['VISITORS']].sum()\n",
    "\n",
    "        # Unire i due DataFrame in base alla colonna 'COD_PRO'\n",
    "        data_com_pro_toll = pd.merge(data_province_toll, data_comuni_toll, suffixes=('_df1', '_df2'), on='COD_PRO', how='outer')\n",
    "\n",
    "        # Sommare i valori della colonna 'visitors' dei due DataFrame\n",
    "        data_com_pro_toll['VISITORS'] = data_com_pro_toll['VISITORS_df1'].fillna(0) + data_com_pro_toll['VISITORS_df2'].fillna(0)\n",
    "        data_com_pro_toll = data_com_pro_toll.drop(['VISITORS_df1','VISITORS_df2'], axis= 1)\n",
    "\n",
    "        data_com_pro_toll = data_com_pro_toll.dropna()\n",
    "\n",
    "        # CALCOLO DEI FLUSSI PER USER CHE NON HANNO UN CASELLO IN PROVINCIA -> DISTRIBUISCO UNIFORMEMENTE PER I CASELLI\n",
    "        # Prendo tutte le province del data_groups[x] -> Ci tolgo quelle con il casello\n",
    "        province = set(data_groups[x]['COD_PRO'].unique())\n",
    "        province_with_toll = set(data_com_pro_toll['COD_PRO'].unique())\n",
    "        province_without_toll = pd.DataFrame({'COD_PRO':list(province - province_with_toll)})\n",
    "        \n",
    "        data_region_toll = pd.merge(data_groups[x], province_without_toll, on=\"COD_PRO\")\n",
    "        \n",
    "        # Sommo il flusso delle province\n",
    "        data_region_toll = data_region_toll.groupby('COD_PRO')[['VISITORS']].sum()\n",
    "        # Uniformo\n",
    "        flow = (data_region_toll['VISITORS'].sum() / len(data_com_pro_toll)).round()\n",
    "        # Sommo\n",
    "        data_com_pro_toll['VISITORS'] = data_com_pro_toll['VISITORS'] + flow\n",
    "\n",
    "        data_groups[x] = data_com_pro_toll\n",
    "        \n",
    "    '''\n",
    "\n",
    "    # CALCOLO DEI FLUSSI DEI COMUNI CON CASELLO\n",
    "    comuni_toll = toll_booths(list_highways[2])\n",
    "    # #Controllo corrispondenze, se trovo corrispondenze calcolo la somma\n",
    "    data_comuni_toll = pd.merge(data_groups[2], comuni_toll, on=\"COMUNE\")\n",
    "    province_with_toll = set(data_groups[2]['COD_PRO'].unique())\n",
    "    comuni_codes = pd.read_csv(filename_codici_istat_comuni, encoding='latin1', usecols=['COMUNE', 'PRO_COM', 'COD_PRO', 'COMUNE_CAPOLUOGO'])\n",
    "    uffa = pd.merge(comuni_codes, comuni_toll, on='COMUNE')\n",
    "\n",
    "    a =  data_groups[data_groups['PRO_COM'] == 28012]\n",
    "    return type(a)\n",
    "\n",
    "    # # CALCOLO DEI FLUSSI DELLE PROVINCE CON CASELLO\n",
    "    # #Altrimenti controllo la provincia\n",
    "    # comuni_codes = pd.read_csv(filename_codici_istat_comuni, encoding='latin1', usecols=['COMUNE', 'PRO_COM', 'COD_PRO', 'COMUNE_CAPOLUOGO'])\n",
    "    # comuni_toll = pd.merge(comuni_toll, comuni_codes, on=\"COMUNE\")\n",
    "    # # Subtraction between two dataframe\n",
    "    # data_without_comuni_toll = data_groups[2].merge(data_comuni_toll, how='outer', indicator=True).query('_merge == \"left_only\"').drop('_merge', axis=1)\n",
    "    # # Prendo le province dei comuni con i caselli\n",
    "    # data_province_toll = data_without_comuni_toll.drop(['COMUNE','PRO_COM'], axis= 1)\n",
    "    # data_province_toll = pd.merge(data_province_toll, comuni_toll, on='COD_PRO')\n",
    "    # data_province_toll = data_province_toll.groupby(['COD_PRO'])[['VISITORS']].sum()\n",
    "\n",
    "    # # Unire i due DataFrame in base alla colonna 'COD_PRO'\n",
    "    # data_com_pro_toll = pd.merge(data_province_toll, data_comuni_toll, suffixes=('_df1', '_df2'), on='COD_PRO', how='outer')\n",
    "\n",
    "    # # Sommare i valori della colonna 'visitors' dei due DataFrame\n",
    "    # data_com_pro_toll['VISITORS'] = data_com_pro_toll['VISITORS_df1'].fillna(0) + data_com_pro_toll['VISITORS_df2'].fillna(0)\n",
    "    # data_com_pro_toll = data_com_pro_toll.drop(['VISITORS_df1','VISITORS_df2'], axis= 1)\n",
    "\n",
    "    # data_com_pro_toll = data_com_pro_toll.dropna()\n",
    "\n",
    "    # # CALCOLO DEI FLUSSI PER USER CHE NON HANNO UN CASELLO IN PROVINCIA -> DISTRIBUISCO UNIFORMEMENTE PER I CASELLI\n",
    "    # # Prendo tutte le province del data_groups[x] -> Ci tolgo quelle con il casello\n",
    "    # province = set(data_groups[2]['COD_PRO'].unique())\n",
    "    # province_with_toll = set(data_com_pro_toll['COD_PRO'].unique())\n",
    "    # province_without_toll = pd.DataFrame({'COD_PRO':list(province - province_with_toll)})\n",
    "    \n",
    "    # data_region_toll = pd.merge(data_groups[2], province_without_toll, on=\"COD_PRO\")\n",
    "    \n",
    "    # # Sommo il flusso delle province\n",
    "    # data_region_toll = data_region_toll.groupby('COD_PRO')[['VISITORS']].sum()\n",
    "    # # Uniformo\n",
    "    # flow = (data_region_toll['VISITORS'].sum() / len(data_com_pro_toll)).round()\n",
    "    # # Sommo\n",
    "    # data_com_pro_toll['VISITORS'] = data_com_pro_toll['VISITORS'] + flow\n",
    "\n",
    "    # data_groups[2] = data_com_pro_toll\n",
    "\n",
    "    return data_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mvisitors_fluxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_distinct_user_day\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 29\u001b[0m, in \u001b[0;36mvisitors_fluxes\u001b[1;34m(file_customers, dow_study)\u001b[0m\n\u001b[0;32m     25\u001b[0m customers_data \u001b[38;5;241m=\u001b[39m customers_data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPRO_COM\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m     27\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(customers_data, user_regions, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPRO_COM\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 29\u001b[0m highways \u001b[38;5;241m=\u001b[39m \u001b[43mpossible_highway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# for i, region_group in enumerate(highways):\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m#     highways[i] = region_group.drop(['COD_PRO','PRO_COM','COD_REG','REGION'], axis= 1)\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m#     plot_highway(region_group)\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m highways\n",
      "Cell \u001b[1;32mIn[50], line 75\u001b[0m, in \u001b[0;36mpossible_highway\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     72\u001b[0m comuni_codes \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(filename_codici_istat_comuni, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m, usecols\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCOMUNE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPRO_COM\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCOD_PRO\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCOMUNE_CAPOLUOGO\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     73\u001b[0m uffa \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(comuni_codes, comuni_toll, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCOMUNE\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data_groups[\u001b[43mdata_groups\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPRO_COM\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m28012\u001b[39m]\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# # CALCOLO DEI FLUSSI DELLE PROVINCE CON CASELLO\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# #Altrimenti controllo la provincia\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# comuni_codes = pd.read_csv(filename_codici_istat_comuni, encoding='latin1', usecols=['COMUNE', 'PRO_COM', 'COD_PRO', 'COMUNE_CAPOLUOGO'])\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    111\u001b[0m \n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# data_groups[2] = data_com_pro_toll\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data_groups\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "visitors_fluxes(filename_distinct_user_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_region(data):\n",
    "    regioni = {\n",
    "        'Abruzzo': 13,\n",
    "        'Basilicata': 17,\n",
    "        'Calabria': 18,\n",
    "        'Campania': 15,\n",
    "        'Emilia Romagna': 8,\n",
    "        'Friuli-Venezia Giulia': 6,\n",
    "        'Lazio': 12,\n",
    "        'Liguria': 7,\n",
    "        'Lombardia': 3,\n",
    "        'Marche': 11,\n",
    "        'Molise': 14,\n",
    "        'Piemonte': 1,\n",
    "        'Puglia': 16,\n",
    "        'Sardegna': 20,\n",
    "        'Sicilia': 19,\n",
    "        'Toscana': 9,\n",
    "        'Trentino-Alto Adige': 4,\n",
    "        'Umbria': 10,\n",
    "        'Valle d\\'Aosta': 2,\n",
    "        'Veneto': 5}\n",
    "    \n",
    "    # Inizializza una lista per memorizzare i nomi delle regioni\n",
    "    region_names = []\n",
    "    \n",
    "    # Itera attraverso ogni riga del DataFrame\n",
    "    for index, row in data.iterrows():\n",
    "        # Ottieni il codice della regione dalla colonna 'COD_REG'\n",
    "        cod_reg = row['COD_REG']\n",
    "        \n",
    "        # Cerca il nome della regione corrispondente al codice nella colonna 'COD_REG'\n",
    "        for region_name, region_code in regioni.items():\n",
    "            if cod_reg == region_code:\n",
    "                # Aggiungi il nome della regione alla lista\n",
    "                region_names.append(region_name)\n",
    "                break\n",
    "        else:\n",
    "            # Se il codice della regione non è presente nel dizionario 'regioni', aggiungi None\n",
    "            region_names.append(None)\n",
    "    \n",
    "    # Aggiungi la lista dei nomi delle regioni come nuova colonna 'REGIONE' nel DataFrame\n",
    "    data['REGION'] = region_names\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visitors_fluxes(file_customers, dow_study=False):\n",
    "    '''\n",
    "    Consider the three main directions:\n",
    "    A13 Roma-Bologna\n",
    "    A4 Torino-Milano\n",
    "    A4 Trieste-Venezia\n",
    "    '''\n",
    "    #Consider the istat province codes\n",
    "    comuni_codes = pd.read_csv(filename_codici_istat_comuni, encoding='latin1', usecols=['COMUNE', 'PRO_COM', 'COD_PRO'])\n",
    "    provinces_codes = pd.read_csv(filename_codici_istat_provincia, encoding='latin1', usecols=['COD_PRO', 'COD_REG'])\n",
    "\n",
    "    codes = pd.merge(comuni_codes, provinces_codes, on='COD_PRO')\n",
    "\n",
    "    if not dow_study:\n",
    "        #Now take the data of the customers and see what's their origin and their destination\n",
    "        customers_data = pd.read_csv(file_customers, encoding='latin1', usecols = ['VISITORS', 'PRO_COM'])\n",
    "    else:\n",
    "        customers_data = pd.read_csv(file_customers, encoding='latin1', usecols = ['VISITORS', 'DOW', 'PRO_COM'])\n",
    "\n",
    "    customers_data.dropna(axis=0, inplace=True)\n",
    "\n",
    "    user_regions = add_region(codes)\n",
    "    \n",
    "    #Join between customers_data and province_codes in this way filters the province_codes in which we're not interested\n",
    "    customers_data = customers_data.groupby('PRO_COM').sum()\n",
    "    \n",
    "    data = pd.merge(customers_data, user_regions, on='PRO_COM')\n",
    "\n",
    "    highways = possible_highway(data)\n",
    "\n",
    "    # for i, region_group in enumerate(highways):\n",
    "    #     highways[i] = region_group.drop(['COD_PRO','PRO_COM','COD_REG','REGION'], axis= 1)\n",
    "\n",
    "    #     plot_highway(region_group)\n",
    "\n",
    "    return highways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_highway(data):\n",
    "    # Estraiamo i dati di interesse dal DataFrame\n",
    "    comuni = data['COMUNE']\n",
    "    visitatori = data['VISITORS']\n",
    "    visitatori_cumulative = data['VISITORS'].cumsum()\n",
    "\n",
    "    # Creiamo il grafico a barre\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(comuni, visitatori, color='skyblue', alpha = 0.5)\n",
    "    plt.plot(comuni, visitatori_cumulative, color='red')\n",
    "\n",
    "    # Aggiungiamo titoli e etichette\n",
    "    plt.title('Numero di visitatori per comune')\n",
    "    plt.xlabel('Comune')\n",
    "    plt.ylabel('Numero di visitatori')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "    # Mostrare il grafico\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'set' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mvisitors_fluxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_distinct_user_day\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 29\u001b[0m, in \u001b[0;36mvisitors_fluxes\u001b[1;34m(file_customers, dow_study)\u001b[0m\n\u001b[0;32m     25\u001b[0m customers_data \u001b[38;5;241m=\u001b[39m customers_data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPRO_COM\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m     27\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(customers_data, user_regions, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPRO_COM\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 29\u001b[0m highways \u001b[38;5;241m=\u001b[39m \u001b[43mpossible_highway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# for i, region_group in enumerate(highways):\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m#     highways[i] = region_group.drop(['COD_PRO','PRO_COM','COD_REG','REGION'], axis= 1)\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m#     plot_highway(region_group)\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m highways\n",
      "Cell \u001b[1;32mIn[8], line 69\u001b[0m, in \u001b[0;36mpossible_highway\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     67\u001b[0m data_comuni_toll \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(data_groups[\u001b[38;5;241m2\u001b[39m], comuni_toll, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCOMUNE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     68\u001b[0m province_with_toll \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(data_groups[\u001b[38;5;241m2\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCOD_PRO\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprovince_with_toll\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# # CALCOLO DEI FLUSSI DELLE PROVINCE CON CASELLO\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# #Altrimenti controllo la provincia\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# comuni_codes = pd.read_csv(filename_codici_istat_comuni, encoding='latin1', usecols=['COMUNE', 'PRO_COM', 'COD_PRO', 'COMUNE_CAPOLUOGO'])\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# data_groups[2] = data_com_pro_toll\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data_groups\n",
      "\u001b[1;31mTypeError\u001b[0m: 'set' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "visitors_fluxes(filename_distinct_user_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_visitors(data, range_of_mobility=\"\"):\n",
    "#     #Counting the flows ov visitors from every city we're interested (both ways)\n",
    "#     data = data.groupby(['PROVINCIA'])[['FLOW']].sum()\n",
    "\n",
    "#     far = True\n",
    "\n",
    "#     if range_of_mobility == \"nearby\":\n",
    "#         far = False\n",
    "\n",
    "#     #Saving the informations per highway\n",
    "#     A13_Roma_Bologna = far * data.FLOW.loc['Roma'] + data.FLOW.loc['Bologna']\n",
    "#     A4_Milano_Torino = data.FLOW.loc['Milano'] + far * data.FLOW.loc['Torino']\n",
    "#     A4_Venezia_Trieste = data.FLOW.loc['Venezia'] + data.FLOW.loc['Trieste']\n",
    "\n",
    "#     highway = [A13_Roma_Bologna, A4_Milano_Torino, A4_Venezia_Trieste]\n",
    "\n",
    "#     #Labels for the plot\n",
    "#     highway_labels = [\"A13 Roma-Bologna\", \"A4 Milano-Torino\", \"A4 Venezia-Trieste\"]\n",
    "    \n",
    "#     # Plotting\n",
    "#     plt.figure(figsize=(10, 4))\n",
    "#     plt.bar(highway_labels, highway, color='skyblue')\n",
    "#     plt.xlabel('Highways')\n",
    "#     plt.ylabel('Visitors'' flow')\n",
    "#     plt.title('Visitors'' flow per highway')\n",
    "#     # Set the scale to avoid the exponential notation \n",
    "#     plt.ticklabel_format(style='plain', axis='y')\n",
    "#     plt.tight_layout()\n",
    "\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.b Mid-range mobility\n",
    "Mobility to/from nearby regions, which are Lombardia, Trentino Alto Adige, Friuli Venezia Giulia, Emilia Romagna.\n",
    "QUESTA SOLUZIONE TIENE CONTO DELLE AUTOSTRADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_visitors(visitors_fluxes(filename_day_od), \"nearby\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Week flow\n",
    "\n",
    "Consider the provinces located on the three directions that are mostly contributing to the flow of weekend visitors and working daily commuters by performing a more detailed study of the fluxes based on the day of the week. Use the data available to provide what you believe is the best possible answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(data1, data2, label):\n",
    "    #Saving the informations per highway\n",
    "    A13_Roma_Bologna = data1.FLOW.loc['Roma'] + data1.FLOW.loc['Bologna']\n",
    "    A4_Milano_Torino = data1.FLOW.loc['Milano'] + data1.FLOW.loc['Torino']\n",
    "    A4_Venezia_Trieste = data1.FLOW.loc['Venezia'] + data1.FLOW.loc['Trieste']\n",
    "\n",
    "    highway1 = [A13_Roma_Bologna, A4_Milano_Torino, A4_Venezia_Trieste]\n",
    "\n",
    "    #Saving the informations per highway\n",
    "    A13_Roma_Bologna = data2.FLOW.loc['Roma'] + data2.FLOW.loc['Bologna']\n",
    "    A4_Milano_Torino = data2.FLOW.loc['Milano'] + data2.FLOW.loc['Torino']\n",
    "    A4_Venezia_Trieste = data2.FLOW.loc['Venezia'] + data2.FLOW.loc['Trieste']\n",
    "\n",
    "    highway2 = [A13_Roma_Bologna, A4_Milano_Torino, A4_Venezia_Trieste]\n",
    "\n",
    "    #Labels for the plot\n",
    "    highway_labels = [\"A13 Roma-Bologna\", \"A4 Milano-Torino\", \"A4 Venezia-Trieste\"]\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    '''\n",
    "    plt.bar(highway_labels, highway, color='skyblue')\n",
    "    plt.xlabel(label)\n",
    "    plt.ylabel('Visitors'' flow')\n",
    "    plt.title('Visitors'' flow in ' + label)\n",
    "    # Set the scale to avoid the exponential notation \n",
    "    plt.ticklabel_format(style='plain', axis='y')\n",
    "    plt.tight_layout()\n",
    "    '''\n",
    "    # Plot dei valori\n",
    "    plt.plot(highway_labels, highway1, marker='o')  # 'marker' specifica il tipo di marker per i punti\n",
    "    plt.plot(highway_labels, highway2, marker='x')  # 'marker' specifica il tipo di marker per i punti\n",
    "    plt.xlabel('X')  # Etichetta asse x\n",
    "    plt.ylabel('Y')  # Etichetta asse y\n",
    "    plt.title('Plot tipo funzione dei valori di un DataFrame')  # Titolo del grafico\n",
    "    plt.grid(False)  # Abilita la griglia\n",
    "\n",
    "    plt.show()\n",
    "    #Fare istogramma grouped bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Seleziono i turisti per le tre direzioni, poi li suddivido in weekend e working days\n",
    "'''\n",
    "def dow_visitors_fluxes(data):\n",
    "    data_weekend = data.loc[(data['DOW'] == \"Domenica\") | (data['DOW'] == \"Sabato\")]\n",
    "\n",
    "    data_working_day = data.loc[~data.index.isin(data_weekend.index)]\n",
    "\n",
    "    # #Join between customers_data and province_codes in this way filters the province_codes in which we're not interested\n",
    "    # data_weekend = pd.merge(weekend_flow, data, on='COD_PRO')\n",
    "    # data_working_day = pd.merge(working_day_flow, data, on='COD_PRO')\n",
    "\n",
    "    #Counting the flows ov visitors from every city we're interested (both ways)\n",
    "    data_weekend = data_weekend.groupby(['PROVINCIA'])[['FLOW']].sum()\n",
    "    data_working_day = data_working_day.groupby(['PROVINCIA'])[['FLOW']].sum()\n",
    "\n",
    "    plotter(data_weekend, data_working_day, \"Weekend\")\n",
    "    # plotter(data_working_day, \"Working \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dow_visitors_fluxes(visitors_fluxes(filename_day_od))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3\n",
    "Plot the distribution of the number of visitors by the distance of the province of origin. Determine which kind of function should be used to describe the distribution.\n",
    "\n",
    "è scritto visitor quindi tolgo i non visitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_visitors_distance(province_data):\n",
    "    distances_data = pd.read_csv(province_data, sep=\"\\t\", encoding=\"UTF-8\", usecols=['DEST_PROCOM', 'KM_TOT'])\n",
    "\n",
    "    distances_data.rename(columns={'DEST_PROCOM': 'PRO_COM'}, inplace=True)\n",
    "\n",
    "    distances_data['KM_TOT'] = distances_data['KM_TOT'].str.replace(',','.')\n",
    "    distances_data['KM_TOT'] = pd.to_numeric(distances_data['KM_TOT'])\n",
    "\n",
    "    distances_data = distances_data.groupby(['PRO_COM'])[['KM_TOT']].mean()\n",
    "\n",
    "    customers_data = pd.read_csv(filename_distinct_user_day, encoding=\"latin1\", usecols=['PRO_COM','VISITORS','CUST_CLASS'])\n",
    "\n",
    "    # Escludo Padova\n",
    "    customers_data['PRO_COM'] = np.where(customers_data['PRO_COM'] == 28060.0 , np.nan,\n",
    "                                np.where(customers_data['CUST_CLASS'] != 'visitor', np.nan, customers_data['PRO_COM']))\n",
    "    customers_data.dropna(axis=0, inplace=True)\n",
    "\n",
    "    # Elimina la colonna \"CUST_CLASS\"\n",
    "    customers_data = customers_data.drop('CUST_CLASS', axis=1)\n",
    "\n",
    "    customers_data = customers_data.groupby(['PRO_COM'])[['VISITORS']].sum()\n",
    "\n",
    "    data = pd.merge(distances_data, customers_data, on='PRO_COM')\n",
    "\n",
    "    data = data.sort_values(['VISITORS'], ascending=False)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_visitors_distance(filename_distance_to_pd_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_visitors_distance(data, zoom=1):\n",
    "    # zoom serve a raggruppare le distanze (50 --> raggruppo le distanze ogni 50 km)\n",
    "    # calcolo il range di raggruppamento\n",
    "    period = int(data['KM_TOT'].max() / zoom)\n",
    "    somma_visitatori = []\n",
    "    for n_group in range(1,period+1):\n",
    "        data_grouped = data[data['KM_TOT'].between((n_group-1)*zoom +1 , n_group * zoom)]\n",
    "        somma_visitatori.append(data_grouped['VISITORS'].sum())\n",
    "    \n",
    "    data_to_plot = pd.DataFrame({'DISTANCE_TO_PD': [km * zoom for km in range(0, period)], 'VISITORS': somma_visitatori})\n",
    "    # Impostare l'indice dopo la creazione del DataFrame\n",
    "    data_to_plot.set_index('DISTANCE_TO_PD', inplace=True)\n",
    "\n",
    "    # data_to_plot['KM_TOT'] = np.where(data_to_plot['KM_TOT'] == 0 , np.nan, data_to_plot['KM_TOT'])\n",
    "    # data_to_plot.dropna(axis=0, inplace=True)\n",
    "\n",
    "    # data_nearby = data.loc[(data['KM_TOT'] < 50)]\n",
    "\n",
    "    # data_far = data.loc[~data.index.isin(data_nearby.index)]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(data_to_plot['VISITORS'], color='blue', alpha=0.7, bins=20)\n",
    "    plt.xlabel('Distance from Padova (KM)')\n",
    "    plt.ylabel('Number of Visitors')\n",
    "    plt.title('Histogram of Distance from Padova')\n",
    "    plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "    plt.show()\n",
    "    #Da riguardare il grafico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_visitors_distance(data_visitors_distance(filename_distance_to_pd_txt), 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 \n",
    "Assuming an analytic form can be used to describe the trend, create a regression or a fit to estimate the expected number of visitors by the distance of the province of origin and the corresponding uncertainties. Illustrate the difference between the resulting regression with respect to the numbers provided by the Vodafone monitoring, and highlight the five most striking discrepancies from the expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Build linear regression model using TV and Radio as predictors\n",
    "# Split data into predictors X and output Y\n",
    "X = np.array(data['KM_TOT']).reshape(-1,1)\n",
    "y = data['VISITORS']\n",
    "\n",
    "# Initialise and fit model\n",
    "lm = LinearRegression()\n",
    "model = lm.fit(X, y)\n",
    "# plot for residual error\n",
    " \n",
    "   \n",
    "plt.scatter(X,y,color='red')\n",
    "plt.plot(X,model.predict(X),color='green')\n",
    "plt.title('Simple Linear Regression')\n",
    "plt.xlabel('Position Level')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
